{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Myopic Mechanic\n",
    "\n",
    "Consider a car owner (let's call him \"Mitt\") facing the problem of when to take his car to the mechanic for service. We assume that Mitt makes one vehicle service decision per month, at the beginning of each month.\n",
    "\n",
    "Let $x$ denote the total miles on the car, and $z$ denote the number of miles since the last service visit (both measured in thousands). Conditional on the total mileage $x$ and the miles since last service $z$, Mitt perceives expected beginning-of-month benefit from operating the car given by\n",
    "$$\n",
    "U(x, z) = a_0 + a_1 x - \\rho_1 z - \\rho_2 \\cdot z \\mathbb{I}[x \\geq 100],\n",
    "$$\n",
    "where $\\rho_1 > 0$ and $\\rho_2 >0 $ represent expected current costs associated with increasing miles since last service visit, including both routine costs and potential breakdown risks, and $\\mathbb{I}[x \\geq 100]$ is an indicator for whether the car has more than 100,000 miles. Meanwhile, visiting the mechanic involves average cost \n",
    "$$\n",
    "C(x, z) = c_0 + c_1 z + c_2 \\cdot z \\mathbb{I}[x \\geq 100],\n",
    "$$\n",
    "where we allow costs of the service visit to increase with miles since last service, but assume that $\\rho_1 > c_1$ and $\\rho_2 > c_2$ (so that as $z$ increases, perceived costs of inaction increase faster than perceived costs of service).\n",
    "\n",
    "If Mitt visits the mechanic, he will incur an expected cost $C(x, z)$ specified above. However, visiting the mechanic resets the number of miles $z$ since the last service visit to zero, allowing Mitt to realize benefit $U(x,0)$ over the rest of the month. \n",
    "\n",
    "At the beginning of every month, Mitt decides whether or not to take his car to the mechanic. \n",
    "Toward this end, he compares the net monthly benefit of taking the car to the mechanic $(d=1)$, \n",
    "\\begin{equation}\n",
    "V_1 = U(x,0) - C(x, z) = a_0 + a_1 x - c_0 - c_1 z - c_2 \\cdot z \\mathbb{I}[x \\geq 100] + \\epsilon_1,\n",
    "\\end{equation}\n",
    "to the  net monthly benefit of operating the car without maintenance ($d=0$),\n",
    "$$\n",
    "V_0 = U(x, z) = a_0 + a_1 x - \\rho_1 z - \\rho_2 \\cdot z \\mathbb{I}[x \\geq 100] + \\epsilon_0,\n",
    "$$\n",
    "where $\\epsilon_0$ and $\\epsilon_1$ are i.i.d. Type 1 Extreme Value utility shocks representing idiosyncratic variation in Mitt's monthly tastes for mechanic visits (much more on these soon!). If $V_1 > V_0$, Mitt takes the car in ($d=1$); otherwise, he doesn't ($d=0$).\n",
    "\n",
    "Note that Mitt is *myopic*, in the sense that he considers only costs and benefits of maintenance within the current month, not prospective benefits in future months from maintaining the car today. We will return to the case of a forward-looking mechanic (Harold Zurcher, the subject of a seminal 1987 paper by John Rust) when we introduce dynamic discrete choice analysis.\n",
    "\n",
    "## Predicted probability of service\n",
    "\n",
    "We first compute the predicted probability that Mitt takes the car for service $(d=1)$ given $x$ and $z$: denote this conditional choice probability by $P(x,z)$. By definition, this probability is given by\n",
    "\\begin{align*}\n",
    "P(x, z) &= P(V_1 - V_0 \\geq 0|x, z) \\\\\n",
    "&= P(-c_0 + (\\rho_1 - c_1)z + (\\rho_2 - c_2) z \\mathbb{I} [x \\geq 100k] \\geq \\epsilon_0 - \\epsilon_1) \\\\\n",
    "&= \\frac{\\exp(-c_0 + (\\rho_1 - c_1) z + (\\rho_2 - c_2) \\cdot z \\mathbb{I} [x \\geq 100])}\n",
    "{1 + \\exp(-c_0 + (\\rho_1 - c_1) z + (\\rho_2 - c_2) \\cdot z \\mathbb{I} [x \\geq 100])},\n",
    "\\end{align*}\n",
    "where the last line follows by the i.i.d. Type I extreme value assumption on the idiosyncratic utility shocks $\\epsilon_0, \\epsilon_1$ (again, much more on this soon!). \n",
    "\n",
    "Note the following features of the predicted probability-of-service function $P(x,z)$:\n",
    "\n",
    "- The terms $a_0 + a_1x$ which appear in both $V_1$ and $V_0$ disappear from the choice probability function. We thus cannot identify either $a_0$ or $a_1$, since these do not affect the choice we observe. In other words, we cannot identify the effects of mileage per se on utility from driving the car; we can only identify effects of mileage insofar as they affect the relative costs of service.\n",
    "\t\n",
    "- Only differences $(\\rho_1 - c_1)$ and $(\\rho_2 - c_2)$ show up in the choice probability function. In this particular example, we can thus only identify the effect of mileage since last service on the differential costs of service versus not.\n",
    "\t\n",
    "- Since the scale of utility is arbitrary, we can identify these differences only up to scale. In assuming that $\\epsilon_0$ and $\\epsilon_1$ were i.i.d. Type 1 EV, we implicitly imposed a normalization on the variances of $\\epsilon_0$ and $\\epsilon_1$. Scaling all terms in utility by any positive constant would lead to the same choice probabilities.\n",
    "\t\n",
    "- Are these identified objects enough? It depends on the counterfactual of interest. For example, if we want to determine how cutting the base cost $c_0$ of a service visit in half would affect the frequency of service, we could do so. If we wanted to determine how a dollar subsidy to service would affect frequency of service, we could not, since in this simple exercise we have no way to convert estimated utilities into dollar terms. For this, we would need to observe variation in price of service, from which we abstract for the moment (as price raises a separate set of endogeneity questions which we will address in detail later). \n",
    "\n",
    "Bearing the above caveats in mind, redefine $\\gamma_0=c_0$, $\\gamma_1 = \\rho_1 - c_1$, and $\\gamma_2 = \\rho_2 - c_2$. These are the primitives which data on Mitt's service choices can identify. \n",
    "\n",
    "## Objectives of the exercise\n",
    "\n",
    "Suppose we observe panel data $(d_{it}, x_{it}, z_{it})_{t=1}^T$ on monthly mileage and service decisions for a collection of individuals $i=1,...,N$, interpreted as a random sample of the population. For simplicity, assume a balanced panel (i.e., the same $T$ for all $i$), although this is inessential.\n",
    "\n",
    "We aim to estimate the parameter vector $\\gamma = (\\gamma_0, \\gamma_1, \\gamma_2)$, assuming that each individual is making auto maintenance choices according to the model described above. Toward this end, we will consider:\n",
    "\n",
    "1. CMLE estimation based on the predicted choice probability function $P(x_{it}, z_{it}; \\gamma)$ derived above.\n",
    "\n",
    "2. GMM estimation based on the conditional mean restriction \n",
    "\t$$E[d_{it} - P(x_{it}, z_{it}; \\gamma) | x_{it}, z_{it}] = 0.$$ \n",
    "\n",
    "In this case, given that we have a fully specified choice model, CMLE will be more efficient, but we also consider GMM for illustrative purposes.\n",
    "\n",
    "To gain a sense for how the estimators compare, we will simulate several Monte Carlo datasets, then explore the performance of each estimator in these simulations. \n",
    "I will provide code for this exercise in Julia, although (for those using other programs) it may be worthwhile to replicate this exercise in other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Drawing data from the model.\n",
    "\n",
    "We first write a few simple functions to generate simulated data from the model above. These use functionality provided by several packages in the Julia language, which we load next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "using Distributions, LinearAlgebra, DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next specify the parameters of the data generating process. We specify the parameters governing service choice as $\\gamma = (5.0, 1.0, 0.2)$. We draw initial mileage $x_{i0}$ from an exponential distribution with mean $60$. We assume the monthly mileage for each individual evolves as $x_{i,t+1} = x_{it} + \\Delta x_{it}$, with $\\Delta x_{it}$ drawn from an exponential distribution with mean $1$. We specify these distribution objects below (note that f_x0 and f_dx defined below are *distribution objects*, which we subsequently feed into a random number generator to draw variables from the relevant distributions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Exponential{Float64}(Î¸=1.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choice probability parameters (object of interest)\n",
    "gamma0 = [5. 1 .2]\n",
    "\n",
    "# Initial mileage distribution (used to simulate data, but not in estimation)\n",
    "f_x0 = Exponential(60.)\n",
    "\n",
    "# Distribution of monthly mileage increment (also used to simulate data, but not in estimate)\n",
    "f_dx = Exponential(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define a Julia function computing the predicted probability that individual $i$ in period $t$ chooses to take their car to a dealership for service. This function will be the core component of our algorithm. To facilitate use in both simulation and estimation, we write the function to take arguments $\\gamma$, the parameters governing choice, and $w = (-1, z, z*\\mathbb{I}[x \\geq 100])$, the vector of observed covariates which affect the choice probability. \n",
    "\n",
    "One technical note in computing predicted choice probabilities: employing $\\gamma$ and $w$ just defined, we may rewrite the predicted choice probability function defined above as\n",
    "\n",
    "\\begin{align}\n",
    "    P(w; \\gamma) &= \\frac{\\exp(w'\\gamma)}{\\exp(0) + \\exp(w'\\gamma)} \\\\\n",
    "    &= \\frac{\\exp(w'\\gamma - \\bar{v})}{\\exp(-\\bar{v}) + \\exp(w'\\gamma - \\bar{v})},\n",
    "\\end{align}\n",
    "\n",
    "where $\\bar{v} = \\max \\{0, w'\\gamma\\}$ is the maximum of the average net utility associated with $d=0$ (i.e., 0.) and the average net utility associated with $d=1$ (i.e., $w'\\gamma$). The latter transformation is useful to ensure numerical stability, as for some values of $w$ and $\\gamma$, the product $w'\\gamma$ could become very large, so that $\\exp(w'\\gamma)$ becomes machine infinity. Normalizing by the maximum of mean utilities prevents such numerical overflow, and ensures that the predicted choice probability $P(w, \\gamma)$ is always numerically stable. This is good programming practice when working with logit models, and is illustrated in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_service_prob (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute predicted probability of taking car for service\n",
    "function predicted_service_prob(gamma, w)\n",
    "    wg = dot(gamma, w)\n",
    "    vbar = max(wg, 0.)\n",
    "    expnormv0 = exp(-vbar)\n",
    "    expnormv1 = exp(wg - vbar)\n",
    "    prob1 = expnormv1 / (expnormv0 + expnormv1)\n",
    "    return(prob1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write a function to draw data from the model above. This function takes arguments nI (the number of individuals) and nT (the number of periods per individual). It returns as output a data frame with columns :I, an individual identifier, :T, a time identifier, :P, the true predicted individual choice probability (not observed in actuality), :D, the observed individual decision, :X, the beginning-of-period cumulative mileage, :Z, the beginning-of-period miles since last service, and :W1-:W3, containing the variables $w_{it}$ for each observation. (The notation :X denotes a *symbol* in Julia -- that is, a unique precompiled identifer, in this case a column name.)\n",
    "\n",
    "Note that we loop over both i and t in simulating data below, computing predicted choice probabilities separately for each individual. This would be a very inefficient construction in Matlab or Python, which tend to slow down dramatically in loops. But the just-in-time compiliation built in to Julia allows loops to execute with little overhead, greatly simplifying efficient coding of inherently recursive operations such as simulation of sequential choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>I</th><th>T</th><th>P</th><th>D</th><th>X</th><th>Z</th><th>W1</th><th>W2</th><th>W3</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>100 rows Ã 9 columns</p><tr><th>1</th><td>1.0</td><td>1.0</td><td>0.0661085</td><td>0.0</td><td>26.3925</td><td>2.35194</td><td>-1.0</td><td>2.35194</td><td>0.0</td></tr><tr><th>2</th><td>1.0</td><td>2.0</td><td>0.0962938</td><td>0.0</td><td>26.8015</td><td>2.7609</td><td>-1.0</td><td>2.7609</td><td>0.0</td></tr><tr><th>3</th><td>1.0</td><td>3.0</td><td>0.097304</td><td>1.0</td><td>26.813</td><td>2.77245</td><td>-1.0</td><td>2.77245</td><td>0.0</td></tr><tr><th>4</th><td>1.0</td><td>4.0</td><td>0.0435267</td><td>0.0</td><td>28.7231</td><td>1.91012</td><td>-1.0</td><td>1.91012</td><td>0.0</td></tr><tr><th>5</th><td>1.0</td><td>5.0</td><td>0.483762</td><td>1.0</td><td>31.748</td><td>4.93503</td><td>-1.0</td><td>4.93503</td><td>0.0</td></tr><tr><th>6</th><td>1.0</td><td>6.0</td><td>0.0252744</td><td>0.0</td><td>33.0957</td><td>1.34764</td><td>-1.0</td><td>1.34764</td><td>0.0</td></tr><tr><th>7</th><td>1.0</td><td>7.0</td><td>0.0300237</td><td>0.0</td><td>33.2728</td><td>1.52471</td><td>-1.0</td><td>1.52471</td><td>0.0</td></tr><tr><th>8</th><td>1.0</td><td>8.0</td><td>0.0604146</td><td>0.0</td><td>34.0038</td><td>2.25579</td><td>-1.0</td><td>2.25579</td><td>0.0</td></tr><tr><th>9</th><td>1.0</td><td>9.0</td><td>0.0868028</td><td>0.0</td><td>34.3947</td><td>2.64669</td><td>-1.0</td><td>2.64669</td><td>0.0</td></tr><tr><th>10</th><td>1.0</td><td>10.0</td><td>0.0879955</td><td>0.0</td><td>34.4097</td><td>2.66164</td><td>-1.0</td><td>2.66164</td><td>0.0</td></tr><tr><th>11</th><td>2.0</td><td>1.0</td><td>0.00753737</td><td>0.0</td><td>151.669</td><td>0.0997361</td><td>-1.0</td><td>0.0997361</td><td>0.0997361</td></tr><tr><th>12</th><td>2.0</td><td>2.0</td><td>0.0106936</td><td>0.0</td><td>151.963</td><td>0.393865</td><td>-1.0</td><td>0.393865</td><td>0.393865</td></tr><tr><th>13</th><td>2.0</td><td>3.0</td><td>0.0119361</td><td>0.0</td><td>152.056</td><td>0.486518</td><td>-1.0</td><td>0.486518</td><td>0.486518</td></tr><tr><th>14</th><td>2.0</td><td>4.0</td><td>0.0119767</td><td>0.0</td><td>152.059</td><td>0.48938</td><td>-1.0</td><td>0.48938</td><td>0.48938</td></tr><tr><th>15</th><td>2.0</td><td>5.0</td><td>0.0380176</td><td>0.0</td><td>153.043</td><td>1.47421</td><td>-1.0</td><td>1.47421</td><td>1.47421</td></tr><tr><th>16</th><td>2.0</td><td>6.0</td><td>0.451574</td><td>1.0</td><td>155.574</td><td>4.00474</td><td>-1.0</td><td>4.00474</td><td>4.00474</td></tr><tr><th>17</th><td>2.0</td><td>7.0</td><td>0.0616463</td><td>0.0</td><td>157.472</td><td>1.89774</td><td>-1.0</td><td>1.89774</td><td>1.89774</td></tr><tr><th>18</th><td>2.0</td><td>8.0</td><td>0.0677529</td><td>0.0</td><td>157.556</td><td>1.98189</td><td>-1.0</td><td>1.98189</td><td>1.98189</td></tr><tr><th>19</th><td>2.0</td><td>9.0</td><td>0.0760196</td><td>0.0</td><td>157.659</td><td>2.08525</td><td>-1.0</td><td>2.08525</td><td>2.08525</td></tr><tr><th>20</th><td>2.0</td><td>10.0</td><td>0.109777</td><td>0.0</td><td>157.996</td><td>2.42248</td><td>-1.0</td><td>2.42248</td><td>2.42248</td></tr><tr><th>21</th><td>3.0</td><td>1.0</td><td>0.0113596</td><td>0.0</td><td>56.2862</td><td>0.533732</td><td>-1.0</td><td>0.533732</td><td>0.0</td></tr><tr><th>22</th><td>3.0</td><td>2.0</td><td>0.0218753</td><td>0.0</td><td>56.9522</td><td>1.19972</td><td>-1.0</td><td>1.19972</td><td>0.0</td></tr><tr><th>23</th><td>3.0</td><td>3.0</td><td>0.0631071</td><td>0.0</td><td>58.0548</td><td>2.30226</td><td>-1.0</td><td>2.30226</td><td>0.0</td></tr><tr><th>24</th><td>3.0</td><td>4.0</td><td>0.106675</td><td>0.0</td><td>58.6273</td><td>2.87484</td><td>-1.0</td><td>2.87484</td><td>0.0</td></tr><tr><th>25</th><td>3.0</td><td>5.0</td><td>0.317059</td><td>1.0</td><td>59.9852</td><td>4.23268</td><td>-1.0</td><td>4.23268</td><td>0.0</td></tr><tr><th>26</th><td>3.0</td><td>6.0</td><td>0.0244079</td><td>1.0</td><td>61.297</td><td>1.31186</td><td>-1.0</td><td>1.31186</td><td>0.0</td></tr><tr><th>27</th><td>3.0</td><td>7.0</td><td>0.00683384</td><td>0.0</td><td>61.318</td><td>0.0209883</td><td>-1.0</td><td>0.0209883</td><td>0.0</td></tr><tr><th>28</th><td>3.0</td><td>8.0</td><td>0.00967489</td><td>0.0</td><td>61.6685</td><td>0.3715</td><td>-1.0</td><td>0.3715</td><td>0.0</td></tr><tr><th>29</th><td>3.0</td><td>9.0</td><td>0.0313445</td><td>0.0</td><td>62.8662</td><td>1.56913</td><td>-1.0</td><td>1.56913</td><td>0.0</td></tr><tr><th>30</th><td>3.0</td><td>10.0</td><td>0.0475209</td><td>0.0</td><td>63.2991</td><td>2.0021</td><td>-1.0</td><td>2.0021</td><td>0.0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& I & T & P & D & X & Z & W1 & W2 & W3\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1.0 & 1.0 & 0.0661085 & 0.0 & 26.3925 & 2.35194 & -1.0 & 2.35194 & 0.0 \\\\\n",
       "\t2 & 1.0 & 2.0 & 0.0962938 & 0.0 & 26.8015 & 2.7609 & -1.0 & 2.7609 & 0.0 \\\\\n",
       "\t3 & 1.0 & 3.0 & 0.097304 & 1.0 & 26.813 & 2.77245 & -1.0 & 2.77245 & 0.0 \\\\\n",
       "\t4 & 1.0 & 4.0 & 0.0435267 & 0.0 & 28.7231 & 1.91012 & -1.0 & 1.91012 & 0.0 \\\\\n",
       "\t5 & 1.0 & 5.0 & 0.483762 & 1.0 & 31.748 & 4.93503 & -1.0 & 4.93503 & 0.0 \\\\\n",
       "\t6 & 1.0 & 6.0 & 0.0252744 & 0.0 & 33.0957 & 1.34764 & -1.0 & 1.34764 & 0.0 \\\\\n",
       "\t7 & 1.0 & 7.0 & 0.0300237 & 0.0 & 33.2728 & 1.52471 & -1.0 & 1.52471 & 0.0 \\\\\n",
       "\t8 & 1.0 & 8.0 & 0.0604146 & 0.0 & 34.0038 & 2.25579 & -1.0 & 2.25579 & 0.0 \\\\\n",
       "\t9 & 1.0 & 9.0 & 0.0868028 & 0.0 & 34.3947 & 2.64669 & -1.0 & 2.64669 & 0.0 \\\\\n",
       "\t10 & 1.0 & 10.0 & 0.0879955 & 0.0 & 34.4097 & 2.66164 & -1.0 & 2.66164 & 0.0 \\\\\n",
       "\t11 & 2.0 & 1.0 & 0.00753737 & 0.0 & 151.669 & 0.0997361 & -1.0 & 0.0997361 & 0.0997361 \\\\\n",
       "\t12 & 2.0 & 2.0 & 0.0106936 & 0.0 & 151.963 & 0.393865 & -1.0 & 0.393865 & 0.393865 \\\\\n",
       "\t13 & 2.0 & 3.0 & 0.0119361 & 0.0 & 152.056 & 0.486518 & -1.0 & 0.486518 & 0.486518 \\\\\n",
       "\t14 & 2.0 & 4.0 & 0.0119767 & 0.0 & 152.059 & 0.48938 & -1.0 & 0.48938 & 0.48938 \\\\\n",
       "\t15 & 2.0 & 5.0 & 0.0380176 & 0.0 & 153.043 & 1.47421 & -1.0 & 1.47421 & 1.47421 \\\\\n",
       "\t16 & 2.0 & 6.0 & 0.451574 & 1.0 & 155.574 & 4.00474 & -1.0 & 4.00474 & 4.00474 \\\\\n",
       "\t17 & 2.0 & 7.0 & 0.0616463 & 0.0 & 157.472 & 1.89774 & -1.0 & 1.89774 & 1.89774 \\\\\n",
       "\t18 & 2.0 & 8.0 & 0.0677529 & 0.0 & 157.556 & 1.98189 & -1.0 & 1.98189 & 1.98189 \\\\\n",
       "\t19 & 2.0 & 9.0 & 0.0760196 & 0.0 & 157.659 & 2.08525 & -1.0 & 2.08525 & 2.08525 \\\\\n",
       "\t20 & 2.0 & 10.0 & 0.109777 & 0.0 & 157.996 & 2.42248 & -1.0 & 2.42248 & 2.42248 \\\\\n",
       "\t21 & 3.0 & 1.0 & 0.0113596 & 0.0 & 56.2862 & 0.533732 & -1.0 & 0.533732 & 0.0 \\\\\n",
       "\t22 & 3.0 & 2.0 & 0.0218753 & 0.0 & 56.9522 & 1.19972 & -1.0 & 1.19972 & 0.0 \\\\\n",
       "\t23 & 3.0 & 3.0 & 0.0631071 & 0.0 & 58.0548 & 2.30226 & -1.0 & 2.30226 & 0.0 \\\\\n",
       "\t24 & 3.0 & 4.0 & 0.106675 & 0.0 & 58.6273 & 2.87484 & -1.0 & 2.87484 & 0.0 \\\\\n",
       "\t25 & 3.0 & 5.0 & 0.317059 & 1.0 & 59.9852 & 4.23268 & -1.0 & 4.23268 & 0.0 \\\\\n",
       "\t26 & 3.0 & 6.0 & 0.0244079 & 1.0 & 61.297 & 1.31186 & -1.0 & 1.31186 & 0.0 \\\\\n",
       "\t27 & 3.0 & 7.0 & 0.00683384 & 0.0 & 61.318 & 0.0209883 & -1.0 & 0.0209883 & 0.0 \\\\\n",
       "\t28 & 3.0 & 8.0 & 0.00967489 & 0.0 & 61.6685 & 0.3715 & -1.0 & 0.3715 & 0.0 \\\\\n",
       "\t29 & 3.0 & 9.0 & 0.0313445 & 0.0 & 62.8662 & 1.56913 & -1.0 & 1.56913 & 0.0 \\\\\n",
       "\t30 & 3.0 & 10.0 & 0.0475209 & 0.0 & 63.2991 & 2.0021 & -1.0 & 2.0021 & 0.0 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "100Ã9 DataFrame. Omitted printing of 3 columns\n",
       "â Row â I       â T       â P          â D       â X       â Z        â\n",
       "â     â \u001b[90mFloat64\u001b[39m â \u001b[90mFloat64\u001b[39m â \u001b[90mFloat64\u001b[39m    â \u001b[90mFloat64\u001b[39m â \u001b[90mFloat64\u001b[39m â \u001b[90mFloat64\u001b[39m  â\n",
       "âââââââ¼ââââââââââ¼ââââââââââ¼âââââââââââââ¼ââââââââââ¼ââââââââââ¼âââââââââââ¤\n",
       "â 1   â 1.0     â 1.0     â 0.0661085  â 0.0     â 26.3925 â 2.35194  â\n",
       "â 2   â 1.0     â 2.0     â 0.0962938  â 0.0     â 26.8015 â 2.7609   â\n",
       "â 3   â 1.0     â 3.0     â 0.097304   â 1.0     â 26.813  â 2.77245  â\n",
       "â 4   â 1.0     â 4.0     â 0.0435267  â 0.0     â 28.7231 â 1.91012  â\n",
       "â 5   â 1.0     â 5.0     â 0.483762   â 1.0     â 31.748  â 4.93503  â\n",
       "â 6   â 1.0     â 6.0     â 0.0252744  â 0.0     â 33.0957 â 1.34764  â\n",
       "â 7   â 1.0     â 7.0     â 0.0300237  â 0.0     â 33.2728 â 1.52471  â\n",
       "â 8   â 1.0     â 8.0     â 0.0604146  â 0.0     â 34.0038 â 2.25579  â\n",
       "â 9   â 1.0     â 9.0     â 0.0868028  â 0.0     â 34.3947 â 2.64669  â\n",
       "â 10  â 1.0     â 10.0    â 0.0879955  â 0.0     â 34.4097 â 2.66164  â\n",
       "â®\n",
       "â 90  â 9.0     â 10.0    â 0.0487164  â 0.0     â 18.8718 â 2.0282   â\n",
       "â 91  â 10.0    â 1.0     â 0.00777248 â 0.0     â 44.8874 â 0.150636 â\n",
       "â 92  â 10.0    â 2.0     â 0.123232   â 0.0     â 47.7746 â 3.03783  â\n",
       "â 93  â 10.0    â 3.0     â 0.153347   â 0.0     â 48.0281 â 3.29141  â\n",
       "â 94  â 10.0    â 4.0     â 0.181088   â 0.0     â 48.2277 â 3.49101  â\n",
       "â 95  â 10.0    â 5.0     â 0.305311   â 0.0     â 48.9146 â 4.17787  â\n",
       "â 96  â 10.0    â 6.0     â 0.353473   â 1.0     â 49.1329 â 4.39619  â\n",
       "â 97  â 10.0    â 7.0     â 0.0226575  â 0.0     â 50.3686 â 1.23566  â\n",
       "â 98  â 10.0    â 8.0     â 0.0455359  â 1.0     â 51.0903 â 1.95735  â\n",
       "â 99  â 10.0    â 9.0     â 0.0202099  â 0.0     â 52.2091 â 1.11883  â\n",
       "â 100 â 10.0    â 10.0    â 0.0365368  â 0.0     â 52.8181 â 1.72779  â"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw data from model (assuming we start following each individual one period after last service)\n",
    "function draw_data(nI, nT)\n",
    "    \n",
    "    # Pre-initialize output matrices\n",
    "    I = zeros(nI*nT)\n",
    "    T = zeros(nI*nT)\n",
    "    P = zeros(nI*nT)\n",
    "    D = zeros(nI*nT)\n",
    "    X = zeros(nI*nT)\n",
    "    Z = zeros(nI*nT)\n",
    "    W = zeros(nI*nT, 3)\n",
    "    w_it = zeros(3)\n",
    "    \n",
    "    # Loop through i by t and simulate data\n",
    "    it = 1\n",
    "    for ii=1:nI\n",
    "        \n",
    "        # initialize x_i, z_i, w_i for this i, x_i0 drawn from f_x0, z_it drawn one period after last service\n",
    "        x_it = rand(f_x0)\n",
    "        z_it = rand(f_dx)\n",
    "        w_it = [-1. z_it z_it*(x_it > 100.)]\n",
    "        \n",
    "        # Loop through periods for this i and update x_i, z_i\n",
    "        for tt=1:nT\n",
    "            \n",
    "            # Fill identifier variables for observation it\n",
    "            I[it] = ii\n",
    "            T[it] = tt\n",
    "            \n",
    "            # Fill beginning-of-period state variables for obs it\n",
    "            X[it] = x_it\n",
    "            Z[it] = z_it\n",
    "            W[it, :] = w_it\n",
    "            \n",
    "            # Compute true predicted probability of service P_it (unobserved)\n",
    "            P[it] = predicted_service_prob(gamma0, w_it)\n",
    "            \n",
    "            # Determine whether service is actually chosen: equivalent to U[0, 1] < P_it\n",
    "            D[it] = rand() < P[it]\n",
    "            \n",
    "            # Increment next period mileage x: x' = x + dx, dx drawn from f_dx\n",
    "            dx = rand(f_dx)\n",
    "            x_it += dx\n",
    "            \n",
    "            # Increment next period z: z' = 0 + dx if service, z' = z + dx otherwise\n",
    "            z_it = (D[it] > 0) ? dx : z_it + dx\n",
    "            \n",
    "            # Update w_it for start of next period and increment counter it\n",
    "            w_it[2] = z_it\n",
    "            w_it[3] = z_it * (x_it >= 100.)\n",
    "            it += 1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Create data frame composed of variables above\n",
    "    #   Note: the exclamation point is Julia syntax for modifying an aspect of an object in place\n",
    "    #   In this case, we first the raw data as a matrix without labels\n",
    "    #   We then update names of each column in the second line\n",
    "    data = DataFrame([I T P D X Z W])\n",
    "    names!(data, [:I; :T; :P; :D; :X; :Z; Symbol.(:W, 1:3)])\n",
    "    return(data)\n",
    "end\n",
    "\n",
    "# Draw a small test dataset to verify function works\n",
    "data = draw_data(10, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: CMLE Estimation\n",
    "\n",
    "We first estimate the model by CMLE. Under the assumptions described above, the choice probability function $P(w_{it}; \\gamma_0)$ completely specifies the marginal density of the choice $d_{it}$ given contemporateous covariates $w_{it}$. Furthermore, by hypothesis, individual $i$'s choice in period $t$ does not depend on lagged realizations of $d_{it}$ and $w_{it}$ for individual $i$, although current $w_{it}$ will generally not be independent of lagged $w_{i,t-1}$ for a given individual. In other words, in the language of panel data analysis, our model is *dynamically complete*, which allows us to apply standard MLE asymptotics.\n",
    "\n",
    "(In this context, dynamic completeness essentially requires that, conditional on current covariates $w_{it}$, current choices $d_{it}$ are independent of past choices $d_{i,t-\\tau}$ and past covariates $w_{i,t-\\tau}$. Since we have assumed independence of utility shocks $\\epsilon_{i0t}, \\epsilon_{i1t}$ across periods, this is true in our context. If these errors were not independent, but the marginal choice probability function $P(w_{it}; \\gamma_0)$ were otherwise correctly specified, we could consider *partial MLE analysis*, which involves maximizing the same marginal objective function, but requires a general quasi-MLE approach asymptotic inference since the conditional information matrix no longer holds. See Wooldridge 13.8 for a detailed discussion.) \n",
    "\n",
    "By definition, the CMLE estimator maximizes the sum of observation-level log-likelihoods, in this case taken across both individuals $i$ and periods $t$. In this case, the observation-level log likelihood is the log of the probability of observing choice $d_{it}$ given the observed covariates $w_{it}$. Bearing in mind that $d_{it}$ is either zero or one, we may write this individual log likelihood concisely as\n",
    "$$\n",
    "    \\ell_i(\\gamma) = d_{it} \\log(P(w_{it}; \\gamma) + (1-d_{it}) \\log(1 - P(w_{it}; \\gamma)).\n",
    "$$\n",
    "Summing across individuals and periods gives the sample log likelihood: \n",
    "$$\n",
    "    \\mathcal{L}_N(\\gamma) = \\sum_{i=1}^N \\sum_{t=1}^T \\ell_i(\\gamma).\n",
    "$$\n",
    "By definition, the CMLE estimator $\\hat{\\gamma}$ maximizes the sample log likelihood $\\mathcal{L}_N(\\gamma)$:\n",
    "$$\n",
    "    \\hat{\\gamma} = \\arg \\max_{\\gamma} \\mathcal{L}_N(\\gamma).\n",
    "$$\n",
    "To find this maximum, we will need functions for calculating the value, gradient (sum of scores), and Hessian of $\\mathcal{L}_N(\\gamma)$. We define these functions next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the sample log-likelihood\n",
    "\n",
    "We first define functions for calculating the individual and sample log-likelihoods. These are straightforward applications of the formulas above. In computing the sample log likelihood, however, we apply one special wrinkle in initializing the output vector which allows Julia to determine the type of the function output dynamically. This allows us to apply automatic differentiation methods to compute the gradient as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_score_hessian (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute observation (it) level log likelihood: log(P_it) if d_it > 0, log(1-P_it) else\n",
    "function period_log_likelihood(gamma, d_it, w_it)\n",
    "    ccp = predicted_service_prob(gamma, w_it)\n",
    "    if d_it > 0\n",
    "        return log(ccp)\n",
    "    else\n",
    "        return log(1 - ccp)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Compute sample log likelihood: summing over periods\n",
    "function sample_log_likelihood(gamma, data)\n",
    "    \n",
    "    # Retrieve relevant columns of data frame in matrix form\n",
    "    D = data[:D]\n",
    "    W = convert(Matrix, data[:, Symbol.(:W, 1:3)])\n",
    "    \n",
    "    # Initialize log likelihood to first value\n",
    "    #  Note: we compute this separately to allow output to be determined by Julia\n",
    "    #  As opposed, for example, to initializing sumll=0., which forces sumll to be a float\n",
    "    #  This doesn't matter for computing the numeric value of the log-likelihood\n",
    "    #  But it is required if we want to use the automatic differentiation methods employed below\n",
    "    sumll = period_log_likelihood(gamma, D[1], W[1,:])\n",
    "    \n",
    "    # Loop over remaining observations and compute overall log likelihood\n",
    "    for it=2:length(D)\n",
    "        sumll += period_log_likelihood(gamma, D[it], W[it, :])\n",
    "    end\n",
    "    \n",
    "    # Return sum log likelihood\n",
    "    return(sumll)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next write a function to compute the value, gradient and Hessian of the log-likelihood function simultaneously. We use the ForwardDiff package in Julia to compute gradients and Hessians automatically -- a powerful tool based on the fact that all machine calculations ultimately boil down to addition, subtraction, multiplication and division (so the chain rule can be applied at the machine operation level). My implementation requires two additional packages, ForwardDiff and DiffResults, whose use is illustrated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff, DiffResults\n",
    "\n",
    "# Compute sample score and Hessian\n",
    "#   Here we use the ForwardDiff package, which allows efficient automatic computation of gradients and hessians\n",
    "function sample_score_hessian(gamma, data)\n",
    "    \n",
    "    # We first initialize a HessianResult structure that allows us to compute objective, score, and Hessian in one shot\n",
    "    #  Here the argument gamma describes the vector with which we aim to take derivatives\n",
    "    hessres = DiffResults.HessianResult(gamma)\n",
    "    \n",
    "    # We now apply the ForwardDiff.hessian! method to compute the gradient and Hessian of the log-likelihood\n",
    "    #  We first specify the log-likelihood as an anonymous function of gamma only\n",
    "    #  We then call ForwardDiff.hessian! to fill the results of hessres\n",
    "    #  Note the !, which specifies that this function will modify one of its arguments\n",
    "    #  In this case, it will modify the hessres structure, which will ultimately contain the outputs\n",
    "    func = g -> sample_log_likelihood(g, data)\n",
    "    ForwardDiff.hessian!(hessres, func, gamma)\n",
    "    \n",
    "    # Finally, we retrieve the objective, score and gradient from the HessRes structure\n",
    "    sumll = DiffResults.value(hessres)\n",
    "    sumscore = DiffResults.gradient(hessres)\n",
    "    sumhessian = DiffResults.hessian(hessres)\n",
    "    return(sumll, sumscore, sumhessian)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000176 seconds (697 allocations: 56.047 KiB)\n",
      "sumll = -34.909437920584004\n",
      "sumscore = [-5.37963 18.8391 2.95215]\n",
      "sumhess = [-8.12719 28.3654 8.52102; 28.3654 -113.227 -32.6098; 8.52102 -32.6098 -32.6098]\n"
     ]
    }
   ],
   "source": [
    "# Test the log-likelihood, score, and Hessian functions\n",
    "sumll, sumscore, sumhess = sample_score_hessian(gamma0, data)\n",
    "@time sumll, sumscore, sumhess = sample_score_hessian(gamma0, data);\n",
    "@show sumll;\n",
    "@show sumscore;\n",
    "@show sumhess;"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
