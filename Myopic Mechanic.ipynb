{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Myopic Mechanic\n",
    "\n",
    "Consider a car owner (let's call him \"Mitt\") facing the problem of when to take his car to the mechanic for service. We assume that Mitt makes one vehicle service decision per month, at the beginning of each month.\n",
    "\n",
    "Let $x$ denote the total miles on the car, and $z$ denote the number of miles since the last service visit (both measured in thousands). Conditional on the total mileage $x$ and the miles since last service $z$, Mitt perceives expected beginning-of-month benefit from operating the car given by\n",
    "$$\n",
    "U(x, z) = a_0 + a_1 x - \\rho_1 z - \\rho_2 \\cdot z \\mathbb{I}[x \\geq 100],\n",
    "$$\n",
    "where $\\rho_1 > 0$ and $\\rho_2 >0 $ represent expected current costs associated with increasing miles since last service visit, including both routine costs and potential breakdown risks, and $\\mathbb{I}[x \\geq 100]$ is an indicator for whether the car has more than 100,000 miles. Meanwhile, visiting the mechanic involves average cost \n",
    "$$\n",
    "C(x, z) = c_0 + c_1 z + c_2 \\cdot z \\mathbb{I}[x \\geq 100],\n",
    "$$\n",
    "where we allow costs of the service visit to increase with miles since last service, but assume that $\\rho_1 > c_1$ and $\\rho_2 > c_2$ (so that as $z$ increases, perceived costs of inaction increase faster than perceived costs of service).\n",
    "\n",
    "If Mitt visits the mechanic, he will incur an expected cost $C(x, z)$ specified above. However, visiting the mechanic resets the number of miles $z$ since the last service visit to zero, allowing Mitt to realize benefit $U(x,0)$ over the rest of the month. \n",
    "\n",
    "At the beginning of every month, Mitt decides whether or not to take his car to the mechanic. \n",
    "Toward this end, he compares the net monthly benefit of taking the car to the mechanic $(d=1)$, \n",
    "\\begin{equation}\n",
    "V_1 = U(x,0) - C(x, z) = a_0 + a_1 x - c_0 - c_1 z - c_2 \\cdot z \\mathbb{I}[x \\geq 100] + \\epsilon_1,\n",
    "\\end{equation}\n",
    "to the  net monthly benefit of operating the car without maintenance ($d=0$),\n",
    "$$\n",
    "V_0 = U(x, z) = a_0 + a_1 x - \\rho_1 z - \\rho_2 \\cdot z \\mathbb{I}[x \\geq 100] + \\epsilon_0,\n",
    "$$\n",
    "where $\\epsilon_0$ and $\\epsilon_1$ are i.i.d. Type 1 Extreme Value utility shocks representing idiosyncratic variation in Mitt's monthly tastes for mechanic visits (much more on these soon!). If $V_1 > V_0$, Mitt takes the car in ($d=1$); otherwise, he doesn't ($d=0$).\n",
    "\n",
    "Note that Mitt is *myopic*, in the sense that he considers only costs and benefits of maintenance within the current month, not prospective benefits in future months from maintaining the car today. We will return to the case of a forward-looking mechanic (Harold Zurcher, the subject of a seminal 1987 paper by John Rust) when we introduce dynamic discrete choice analysis.\n",
    "\n",
    "## Predicted probability of service\n",
    "\n",
    "We first compute the predicted probability that Mitt takes the car for service $(d=1)$ given $x$ and $z$: denote this conditional choice probability by $P(x,z)$. By definition, this probability is given by\n",
    "\\begin{align}\n",
    "P(x, z) &= P(V_1 - V_0 \\geq 0|x, z) \\\\\n",
    "&= P(-c_0 + (\\rho_1 - c_1)z + (\\rho_2 - c_2) z \\mathbb{I} [x \\geq 100k] \\geq \\epsilon_0 - \\epsilon_1) \\\\\n",
    "&= \\frac{\\exp(-c_0 + (\\rho_1 - c_1) z + (\\rho_2 - c_2) \\cdot z \\mathbb{I} [x \\geq 100])}\n",
    "{1 + \\exp(-c_0 + (\\rho_1 - c_1) z + (\\rho_2 - c_2) \\cdot z \\mathbb{I} [x \\geq 100])},\n",
    "\\end{align}\n",
    "where the last line follows by the i.i.d. Type I extreme value assumption on the idiosyncratic utility shocks $\\epsilon_0, \\epsilon_1$ (again, much more on this soon!). \n",
    "\n",
    "Note the following features of the predicted probability-of-service function $P(x,z)$:\n",
    "\n",
    "- The terms $a_0 + a_1x$ which appear in both $V_1$ and $V_0$ disappear from the choice probability function. We thus cannot identify either $a_0$ or $a_1$, since these do not affect the choice we observe. In other words, we cannot identify the effects of mileage per se on utility from driving the car; we can only identify effects of mileage insofar as they affect the relative costs of service.\n",
    "\t\n",
    "- Only differences $(\\rho_1 - c_1)$ and $(\\rho_2 - c_2)$ show up in the choice probability function. In this particular example, we can thus only identify the effect of mileage since last service on the differential costs of service versus not.\n",
    "\t\n",
    "- Since the scale of utility is arbitrary, we can identify these differences only up to scale. In assuming that $\\epsilon_0$ and $\\epsilon_1$ were i.i.d. Type 1 EV, we implicitly imposed a normalization on the variances of $\\epsilon_0$ and $\\epsilon_1$. Scaling all terms in utility by any positive constant would lead to the same choice probabilities.\n",
    "\t\n",
    "- Are these identified objects enough? It depends on the counterfactual of interest. For example, if we want to determine how cutting the base cost $c_0$ of a service visit in half would affect the frequency of service, we could do so. If we wanted to determine how a dollar subsidy to service would affect frequency of service, we could not, since in this simple exercise we have no way to convert estimated utilities into dollar terms. For this, we would need to observe variation in price of service, from which we abstract for the moment (as price raises a separate set of endogeneity questions which we will address in detail later). \n",
    "\n",
    "Bearing the above caveats in mind, redefine $\\gamma_0=c_0$, $\\gamma_1 = \\rho_1 - c_1$, and $\\gamma_2 = \\rho_2 - c_2$. These are the primitives which data on Mitt's service choices can identify. \n",
    "\n",
    "## Objectives of the exercise\n",
    "\n",
    "Suppose we observe panel data $(d_{it}, x_{it}, z_{it})_{t=1}^T$ on monthly mileage and service decisions for a collection of individuals $i=1,...,N$, interpreted as a random sample of the population. For simplicity, assume a balanced panel (i.e., the same $T$ for all $i$), although this is inessential.\n",
    "\n",
    "We aim to estimate the parameter vector $\\gamma = (\\gamma_0, \\gamma_1, \\gamma_2)$, assuming that each individual is making auto maintenance choices according to the model described above. Toward this end, we will consider:\n",
    "\n",
    "1. CMLE estimation based on the predicted choice probability function $P(x_{it}, z_{it}; \\gamma)$ derived above.\n",
    "\n",
    "2. GMM estimation based on the conditional mean restriction \n",
    "\t$$E[d_{it} - P(x_{it}, z_{it}; \\gamma) | x_{it}, z_{it}] = 0.$$ \n",
    "\n",
    "In this case, given that we have a fully specified choice model, CMLE will be more efficient, but we also consider GMM for illustrative purposes.\n",
    "\n",
    "To gain a sense for how the estimators compare, we will simulate several Monte Carlo datasets, then explore the performance of each estimator in these simulations. \n",
    "I will provide code for this exercise in Julia, although (for those using other programs) it may be worthwhile to replicate this exercise in other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Drawing data from the model.\n",
    "\n",
    "We first write a few simple functions to generate simulated data from the model above. These use functionality provided by several packages in the Julia language, which we load next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "using Distributions, LinearAlgebra, DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next specify the parameters of the data generating process. We specify the parameters governing service choice as $\\gamma = (5.0, 1.0, 0.2)$. We draw initial mileage $x_{i0}$ from an exponential distribution with mean $60$. We assume the monthly mileage for each individual evolves as $x_{i,t+1} = x_{it} + \\Delta x_{it}$, with $\\Delta x_{it}$ drawn from an exponential distribution with mean $1$. We specify these distribution objects below (note that f_x0 and f_dx defined below are *distribution objects*, which we subsequently feed into a random number generator to draw variables from the relevant distributions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(gamma0) = (3,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Exponential{Float64}(θ=1.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choice probability parameters (object of interest)\n",
    "gamma0 = [5.; 1; .2]\n",
    "@show size(gamma0)\n",
    "\n",
    "# Initial mileage distribution (used to simulate data, but not in estimation)\n",
    "f_x0 = Exponential(60.)\n",
    "\n",
    "# Distribution of monthly mileage increment (also used to simulate data, but not in estimate)\n",
    "f_dx = Exponential(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define a Julia function computing the predicted probability that individual $i$ in period $t$ chooses to take their car to a dealership for service. This function will be the core component of our algorithm. To facilitate use in both simulation and estimation, we write the function to take arguments $\\gamma$, the parameters governing choice, and $w = (-1, z, z*\\mathbb{I}[x \\geq 100])$, the vector of observed covariates which affect the choice probability. \n",
    "\n",
    "One technical note in computing predicted choice probabilities: employing $\\gamma$ and $w$ just defined, we may rewrite the predicted choice probability function defined above as\n",
    "\n",
    "\\begin{align}\n",
    "    P(w; \\gamma) &= \\frac{\\exp(w'\\gamma)}{\\exp(0) + \\exp(w'\\gamma)} \\\\\n",
    "    &= \\frac{\\exp(w'\\gamma - \\bar{v})}{\\exp(-\\bar{v}) + \\exp(w'\\gamma - \\bar{v})},\n",
    "\\end{align}\n",
    "\n",
    "where $\\bar{v} = \\max \\{0, w'\\gamma\\}$ is the maximum of the average net utility associated with $d=0$ (i.e., 0.) and the average net utility associated with $d=1$ (i.e., $w'\\gamma$). The latter transformation is useful to ensure numerical stability, as for some values of $w$ and $\\gamma$, the product $w'\\gamma$ could become very large, so that $\\exp(w'\\gamma)$ becomes machine infinity. Normalizing by the maximum of mean utilities prevents such numerical overflow, and ensures that the predicted choice probability $P(w, \\gamma)$ is always numerically stable. This is good programming practice when working with logit models, and is illustrated in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_service_prob (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute predicted probability of taking car for service\n",
    "function predicted_service_prob(gamma, w)\n",
    "    wg = dot(gamma, w)\n",
    "    vbar = max(wg, 0.)\n",
    "    expnormv0 = exp(-vbar)\n",
    "    expnormv1 = exp(wg - vbar)\n",
    "    prob1 = expnormv1 / (expnormv0 + expnormv1)\n",
    "    return(prob1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write a function to draw data from the model above. This function takes arguments nI (the number of individuals) and nT (the number of periods per individual). It returns as output a data frame with columns :I, an individual identifier, :T, a time identifier, :P, the true predicted individual choice probability (not observed in actuality), :D, the observed individual decision, :X, the beginning-of-period cumulative mileage, :Z, the beginning-of-period miles since last service, and :W1-:W3, containing the variables $w_{it}$ for each observation. (The notation :X denotes a *symbol* in Julia -- that is, a unique precompiled identifer, in this case a column name.)\n",
    "\n",
    "Note that we loop over both i and t in simulating data below, computing predicted choice probabilities separately for each individual. This would be a very inefficient construction in Matlab or Python, which tend to slow down dramatically in loops. But the just-in-time compiliation built in to Julia allows loops to execute with little overhead, greatly simplifying efficient coding of inherently recursive operations such as simulation of sequential choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "draw_data (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw data from model (assuming we start following each individual one period after last service)\n",
    "function draw_data(nI, nT)\n",
    "    \n",
    "    # Pre-initialize output matrices\n",
    "    I = zeros(nI*nT)\n",
    "    T = zeros(nI*nT)\n",
    "    P = zeros(nI*nT)\n",
    "    D = zeros(nI*nT)\n",
    "    X = zeros(nI*nT)\n",
    "    Z = zeros(nI*nT)\n",
    "    W = zeros(nI*nT, 3)\n",
    "    w_it = zeros(3)\n",
    "    \n",
    "    # Loop through i by t and simulate data\n",
    "    it = 1\n",
    "    for ii=1:nI\n",
    "        \n",
    "        # initialize x_i, z_i, w_i for this i, x_i0 drawn from f_x0, z_it drawn one period after last service\n",
    "        x_it = rand(f_x0)\n",
    "        z_it = rand(f_dx)\n",
    "        w_it = [-1. z_it z_it*(x_it > 100.)]\n",
    "        \n",
    "        # Loop through periods for this i and update x_i, z_i\n",
    "        for tt=1:nT\n",
    "            \n",
    "            # Fill identifier variables for observation it\n",
    "            I[it] = ii\n",
    "            T[it] = tt\n",
    "            \n",
    "            # Fill beginning-of-period state variables for obs it\n",
    "            X[it] = x_it\n",
    "            Z[it] = z_it\n",
    "            W[it, :] = w_it\n",
    "            \n",
    "            # Compute true predicted probability of service P_it (unobserved)\n",
    "            P[it] = predicted_service_prob(gamma0, w_it)\n",
    "            \n",
    "            # Determine whether service is actually chosen: equivalent to U[0, 1] < P_it\n",
    "            D[it] = rand() < P[it]\n",
    "            \n",
    "            # Increment next period mileage x: x' = x + dx, dx drawn from f_dx\n",
    "            dx = rand(f_dx)\n",
    "            x_it += dx\n",
    "            \n",
    "            # Increment next period z: z' = 0 + dx if service, z' = z + dx otherwise\n",
    "            z_it = (D[it] > 0) ? dx : z_it + dx\n",
    "            \n",
    "            # Update w_it for start of next period and increment counter it\n",
    "            w_it[2] = z_it\n",
    "            w_it[3] = z_it * (x_it >= 100.)\n",
    "            it += 1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Create data frame composed of variables above\n",
    "    #   Note: the exclamation point is Julia syntax for modifying an aspect of an object in place\n",
    "    #   In this case, we first the raw data as a matrix without labels\n",
    "    #   We then update names of each column in the second line\n",
    "    data = DataFrame([I T P D X Z W])\n",
    "    rename!(data, [:I; :T; :P; :D; :X; :Z; Symbol.(:W, 1:3)])\n",
    "    return(data)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>I</th><th>T</th><th>P</th><th>D</th><th>X</th><th>Z</th><th>W1</th><th>W2</th><th>W3</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>10,000 rows × 9 columns</p><tr><th>1</th><td>1.0</td><td>1.0</td><td>0.0139515</td><td>0.0</td><td>7.68699</td><td>0.741881</td><td>-1.0</td><td>0.741881</td><td>0.0</td></tr><tr><th>2</th><td>1.0</td><td>2.0</td><td>0.0302285</td><td>0.0</td><td>8.47684</td><td>1.53173</td><td>-1.0</td><td>1.53173</td><td>0.0</td></tr><tr><th>3</th><td>1.0</td><td>3.0</td><td>0.186092</td><td>1.0</td><td>10.4695</td><td>3.52439</td><td>-1.0</td><td>3.52439</td><td>0.0</td></tr><tr><th>4</th><td>1.0</td><td>4.0</td><td>0.0126494</td><td>0.0</td><td>11.1121</td><td>0.642581</td><td>-1.0</td><td>0.642581</td><td>0.0</td></tr><tr><th>5</th><td>1.0</td><td>5.0</td><td>0.0439209</td><td>0.0</td><td>12.3891</td><td>1.91955</td><td>-1.0</td><td>1.91955</td><td>0.0</td></tr><tr><th>6</th><td>1.0</td><td>6.0</td><td>0.135114</td><td>0.0</td><td>13.613</td><td>3.14352</td><td>-1.0</td><td>3.14352</td><td>0.0</td></tr><tr><th>7</th><td>1.0</td><td>7.0</td><td>0.331878</td><td>1.0</td><td>14.7698</td><td>4.3003</td><td>-1.0</td><td>4.3003</td><td>0.0</td></tr><tr><th>8</th><td>1.0</td><td>8.0</td><td>0.00862364</td><td>0.0</td><td>15.0252</td><td>0.255413</td><td>-1.0</td><td>0.255413</td><td>0.0</td></tr><tr><th>9</th><td>1.0</td><td>9.0</td><td>0.011274</td><td>0.0</td><td>15.2959</td><td>0.526086</td><td>-1.0</td><td>0.526086</td><td>0.0</td></tr><tr><th>10</th><td>1.0</td><td>10.0</td><td>0.0290085</td><td>0.0</td><td>16.2591</td><td>1.48927</td><td>-1.0</td><td>1.48927</td><td>0.0</td></tr><tr><th>11</th><td>2.0</td><td>1.0</td><td>0.0192738</td><td>0.0</td><td>23.9957</td><td>1.07045</td><td>-1.0</td><td>1.07045</td><td>0.0</td></tr><tr><th>12</th><td>2.0</td><td>2.0</td><td>0.0442661</td><td>0.0</td><td>24.853</td><td>1.92774</td><td>-1.0</td><td>1.92774</td><td>0.0</td></tr><tr><th>13</th><td>2.0</td><td>3.0</td><td>0.048839</td><td>0.0</td><td>24.9561</td><td>2.03085</td><td>-1.0</td><td>2.03085</td><td>0.0</td></tr><tr><th>14</th><td>2.0</td><td>4.0</td><td>0.453488</td><td>0.0</td><td>27.7387</td><td>4.81341</td><td>-1.0</td><td>4.81341</td><td>0.0</td></tr><tr><th>15</th><td>2.0</td><td>5.0</td><td>0.467904</td><td>1.0</td><td>27.7967</td><td>4.87144</td><td>-1.0</td><td>4.87144</td><td>0.0</td></tr><tr><th>16</th><td>2.0</td><td>6.0</td><td>0.0121737</td><td>0.0</td><td>28.4005</td><td>0.60377</td><td>-1.0</td><td>0.60377</td><td>0.0</td></tr><tr><th>17</th><td>2.0</td><td>7.0</td><td>0.0214364</td><td>0.0</td><td>28.9757</td><td>1.17901</td><td>-1.0</td><td>1.17901</td><td>0.0</td></tr><tr><th>18</th><td>2.0</td><td>8.0</td><td>0.0343684</td><td>0.0</td><td>29.461</td><td>1.66435</td><td>-1.0</td><td>1.66435</td><td>0.0</td></tr><tr><th>19</th><td>2.0</td><td>9.0</td><td>0.0373892</td><td>0.0</td><td>29.5484</td><td>1.75173</td><td>-1.0</td><td>1.75173</td><td>0.0</td></tr><tr><th>20</th><td>2.0</td><td>10.0</td><td>0.0437476</td><td>0.0</td><td>29.7121</td><td>1.91542</td><td>-1.0</td><td>1.91542</td><td>0.0</td></tr><tr><th>21</th><td>3.0</td><td>1.0</td><td>0.0117213</td><td>0.0</td><td>164.73</td><td>0.471203</td><td>-1.0</td><td>0.471203</td><td>0.471203</td></tr><tr><th>22</th><td>3.0</td><td>2.0</td><td>0.0119514</td><td>0.0</td><td>164.747</td><td>0.487597</td><td>-1.0</td><td>0.487597</td><td>0.487597</td></tr><tr><th>23</th><td>3.0</td><td>3.0</td><td>0.0403245</td><td>0.0</td><td>165.785</td><td>1.5253</td><td>-1.0</td><td>1.5253</td><td>1.5253</td></tr><tr><th>24</th><td>3.0</td><td>4.0</td><td>0.219593</td><td>0.0</td><td>167.369</td><td>3.10997</td><td>-1.0</td><td>3.10997</td><td>3.10997</td></tr><tr><th>25</th><td>3.0</td><td>5.0</td><td>0.294308</td><td>0.0</td><td>167.697</td><td>3.43787</td><td>-1.0</td><td>3.43787</td><td>3.43787</td></tr><tr><th>26</th><td>3.0</td><td>6.0</td><td>0.978519</td><td>1.0</td><td>171.608</td><td>7.34904</td><td>-1.0</td><td>7.34904</td><td>7.34904</td></tr><tr><th>27</th><td>3.0</td><td>7.0</td><td>0.00746173</td><td>0.0</td><td>171.7</td><td>0.091268</td><td>-1.0</td><td>0.091268</td><td>0.091268</td></tr><tr><th>28</th><td>3.0</td><td>8.0</td><td>0.0107591</td><td>0.0</td><td>172.007</td><td>0.399013</td><td>-1.0</td><td>0.399013</td><td>0.399013</td></tr><tr><th>29</th><td>3.0</td><td>9.0</td><td>0.011461</td><td>0.0</td><td>172.061</td><td>0.452271</td><td>-1.0</td><td>0.452271</td><td>0.452271</td></tr><tr><th>30</th><td>3.0</td><td>10.0</td><td>0.0156859</td><td>0.0</td><td>172.326</td><td>0.717348</td><td>-1.0</td><td>0.717348</td><td>0.717348</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& I & T & P & D & X & Z & W1 & W2 & W3\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1.0 & 1.0 & 0.0139515 & 0.0 & 7.68699 & 0.741881 & -1.0 & 0.741881 & 0.0 \\\\\n",
       "\t2 & 1.0 & 2.0 & 0.0302285 & 0.0 & 8.47684 & 1.53173 & -1.0 & 1.53173 & 0.0 \\\\\n",
       "\t3 & 1.0 & 3.0 & 0.186092 & 1.0 & 10.4695 & 3.52439 & -1.0 & 3.52439 & 0.0 \\\\\n",
       "\t4 & 1.0 & 4.0 & 0.0126494 & 0.0 & 11.1121 & 0.642581 & -1.0 & 0.642581 & 0.0 \\\\\n",
       "\t5 & 1.0 & 5.0 & 0.0439209 & 0.0 & 12.3891 & 1.91955 & -1.0 & 1.91955 & 0.0 \\\\\n",
       "\t6 & 1.0 & 6.0 & 0.135114 & 0.0 & 13.613 & 3.14352 & -1.0 & 3.14352 & 0.0 \\\\\n",
       "\t7 & 1.0 & 7.0 & 0.331878 & 1.0 & 14.7698 & 4.3003 & -1.0 & 4.3003 & 0.0 \\\\\n",
       "\t8 & 1.0 & 8.0 & 0.00862364 & 0.0 & 15.0252 & 0.255413 & -1.0 & 0.255413 & 0.0 \\\\\n",
       "\t9 & 1.0 & 9.0 & 0.011274 & 0.0 & 15.2959 & 0.526086 & -1.0 & 0.526086 & 0.0 \\\\\n",
       "\t10 & 1.0 & 10.0 & 0.0290085 & 0.0 & 16.2591 & 1.48927 & -1.0 & 1.48927 & 0.0 \\\\\n",
       "\t11 & 2.0 & 1.0 & 0.0192738 & 0.0 & 23.9957 & 1.07045 & -1.0 & 1.07045 & 0.0 \\\\\n",
       "\t12 & 2.0 & 2.0 & 0.0442661 & 0.0 & 24.853 & 1.92774 & -1.0 & 1.92774 & 0.0 \\\\\n",
       "\t13 & 2.0 & 3.0 & 0.048839 & 0.0 & 24.9561 & 2.03085 & -1.0 & 2.03085 & 0.0 \\\\\n",
       "\t14 & 2.0 & 4.0 & 0.453488 & 0.0 & 27.7387 & 4.81341 & -1.0 & 4.81341 & 0.0 \\\\\n",
       "\t15 & 2.0 & 5.0 & 0.467904 & 1.0 & 27.7967 & 4.87144 & -1.0 & 4.87144 & 0.0 \\\\\n",
       "\t16 & 2.0 & 6.0 & 0.0121737 & 0.0 & 28.4005 & 0.60377 & -1.0 & 0.60377 & 0.0 \\\\\n",
       "\t17 & 2.0 & 7.0 & 0.0214364 & 0.0 & 28.9757 & 1.17901 & -1.0 & 1.17901 & 0.0 \\\\\n",
       "\t18 & 2.0 & 8.0 & 0.0343684 & 0.0 & 29.461 & 1.66435 & -1.0 & 1.66435 & 0.0 \\\\\n",
       "\t19 & 2.0 & 9.0 & 0.0373892 & 0.0 & 29.5484 & 1.75173 & -1.0 & 1.75173 & 0.0 \\\\\n",
       "\t20 & 2.0 & 10.0 & 0.0437476 & 0.0 & 29.7121 & 1.91542 & -1.0 & 1.91542 & 0.0 \\\\\n",
       "\t21 & 3.0 & 1.0 & 0.0117213 & 0.0 & 164.73 & 0.471203 & -1.0 & 0.471203 & 0.471203 \\\\\n",
       "\t22 & 3.0 & 2.0 & 0.0119514 & 0.0 & 164.747 & 0.487597 & -1.0 & 0.487597 & 0.487597 \\\\\n",
       "\t23 & 3.0 & 3.0 & 0.0403245 & 0.0 & 165.785 & 1.5253 & -1.0 & 1.5253 & 1.5253 \\\\\n",
       "\t24 & 3.0 & 4.0 & 0.219593 & 0.0 & 167.369 & 3.10997 & -1.0 & 3.10997 & 3.10997 \\\\\n",
       "\t25 & 3.0 & 5.0 & 0.294308 & 0.0 & 167.697 & 3.43787 & -1.0 & 3.43787 & 3.43787 \\\\\n",
       "\t26 & 3.0 & 6.0 & 0.978519 & 1.0 & 171.608 & 7.34904 & -1.0 & 7.34904 & 7.34904 \\\\\n",
       "\t27 & 3.0 & 7.0 & 0.00746173 & 0.0 & 171.7 & 0.091268 & -1.0 & 0.091268 & 0.091268 \\\\\n",
       "\t28 & 3.0 & 8.0 & 0.0107591 & 0.0 & 172.007 & 0.399013 & -1.0 & 0.399013 & 0.399013 \\\\\n",
       "\t29 & 3.0 & 9.0 & 0.011461 & 0.0 & 172.061 & 0.452271 & -1.0 & 0.452271 & 0.452271 \\\\\n",
       "\t30 & 3.0 & 10.0 & 0.0156859 & 0.0 & 172.326 & 0.717348 & -1.0 & 0.717348 & 0.717348 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "10000×9 DataFrame. Omitted printing of 3 columns\n",
       "│ Row   │ I       │ T       │ P          │ D       │ X       │ Z        │\n",
       "│       │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m    │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m  │\n",
       "├───────┼─────────┼─────────┼────────────┼─────────┼─────────┼──────────┤\n",
       "│ 1     │ 1.0     │ 1.0     │ 0.0139515  │ 0.0     │ 7.68699 │ 0.741881 │\n",
       "│ 2     │ 1.0     │ 2.0     │ 0.0302285  │ 0.0     │ 8.47684 │ 1.53173  │\n",
       "│ 3     │ 1.0     │ 3.0     │ 0.186092   │ 1.0     │ 10.4695 │ 3.52439  │\n",
       "│ 4     │ 1.0     │ 4.0     │ 0.0126494  │ 0.0     │ 11.1121 │ 0.642581 │\n",
       "│ 5     │ 1.0     │ 5.0     │ 0.0439209  │ 0.0     │ 12.3891 │ 1.91955  │\n",
       "│ 6     │ 1.0     │ 6.0     │ 0.135114   │ 0.0     │ 13.613  │ 3.14352  │\n",
       "│ 7     │ 1.0     │ 7.0     │ 0.331878   │ 1.0     │ 14.7698 │ 4.3003   │\n",
       "│ 8     │ 1.0     │ 8.0     │ 0.00862364 │ 0.0     │ 15.0252 │ 0.255413 │\n",
       "│ 9     │ 1.0     │ 9.0     │ 0.011274   │ 0.0     │ 15.2959 │ 0.526086 │\n",
       "│ 10    │ 1.0     │ 10.0    │ 0.0290085  │ 0.0     │ 16.2591 │ 1.48927  │\n",
       "⋮\n",
       "│ 9990  │ 999.0   │ 10.0    │ 0.0329914  │ 0.0     │ 86.2717 │ 1.62204  │\n",
       "│ 9991  │ 1000.0  │ 1.0     │ 0.0144435  │ 0.0     │ 86.2134 │ 0.777039 │\n",
       "│ 9992  │ 1000.0  │ 2.0     │ 0.0511349  │ 0.0     │ 87.5155 │ 2.0792   │\n",
       "│ 9993  │ 1000.0  │ 3.0     │ 0.114975   │ 0.0     │ 88.3954 │ 2.9591   │\n",
       "│ 9994  │ 1000.0  │ 4.0     │ 0.198332   │ 0.0     │ 89.0396 │ 3.60325  │\n",
       "│ 9995  │ 1000.0  │ 5.0     │ 0.515821   │ 1.0     │ 90.4996 │ 5.0633   │\n",
       "│ 9996  │ 1000.0  │ 6.0     │ 0.00845397 │ 0.0     │ 90.735  │ 0.235371 │\n",
       "│ 9997  │ 1000.0  │ 7.0     │ 0.024396   │ 1.0     │ 91.811  │ 1.31136  │\n",
       "│ 9998  │ 1000.0  │ 8.0     │ 0.0125516  │ 0.0     │ 92.4457 │ 0.634727 │\n",
       "│ 9999  │ 1000.0  │ 9.0     │ 0.0317441  │ 1.0     │ 93.3932 │ 1.58221  │\n",
       "│ 10000 │ 1000.0  │ 10.0    │ 0.579589   │ 1.0     │ 98.7143 │ 5.32109  │"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw a representative dataset\n",
    "data = draw_data(1000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: CMLE Estimation\n",
    "\n",
    "We first estimate the model by CMLE. Under the assumptions described above, the choice probability function $P(w_{it}; \\gamma_0)$ completely specifies the marginal density of the choice $d_{it}$ given contemporateous covariates $w_{it}$. Furthermore, by hypothesis, individual $i$'s choice in period $t$ does not depend on lagged realizations of $d_{it}$ and $w_{it}$ for individual $i$, although current $w_{it}$ will generally not be independent of lagged $w_{i,t-1}$ for a given individual. In other words, in the language of panel data analysis, our model is *dynamically complete*, which allows us to apply standard MLE asymptotics.\n",
    "\n",
    "(In this context, dynamic completeness essentially requires that, conditional on current covariates $w_{it}$, current choices $d_{it}$ are independent of past choices $d_{i,t-\\tau}$ and past covariates $w_{i,t-\\tau}$. Since we have assumed independence of utility shocks $\\epsilon_{i0t}, \\epsilon_{i1t}$ across periods, this is true in our context. If these errors were not independent, but the marginal choice probability function $P(w_{it}; \\gamma_0)$ were otherwise correctly specified, we could consider *partial MLE analysis*, which involves maximizing the same marginal objective function, but requires a general quasi-MLE approach asymptotic inference since the conditional information matrix no longer holds. See Wooldridge 13.8 for a detailed discussion.) \n",
    "\n",
    "By definition, the CMLE estimator maximizes the sum of observation-level log-likelihoods, in this case taken across both individuals $i$ and periods $t$. In this case, the observation-level log likelihood is the log of the probability of observing choice $d_{it}$ given the observed covariates $w_{it}$. Bearing in mind that $d_{it}$ is either zero or one, we may write this individual log likelihood concisely as\n",
    "$$\n",
    "    \\ell_i(\\gamma) = d_{it} \\log(P(w_{it}; \\gamma)) + (1-d_{it}) \\log(1 - P(w_{it}; \\gamma)).\n",
    "$$\n",
    "Summing across individuals and periods gives the sample log likelihood: \n",
    "$$\n",
    "    \\mathcal{L}_N(\\gamma) = \\sum_{i=1}^N \\sum_{t=1}^T \\ell_i(\\gamma).\n",
    "$$\n",
    "By definition, the CMLE estimator $\\hat{\\gamma}$ maximizes the sample log likelihood $\\mathcal{L}_N(\\gamma)$:\n",
    "$$\n",
    "    \\hat{\\gamma} = \\arg \\max_{\\gamma} \\mathcal{L}_N(\\gamma).\n",
    "$$\n",
    "To find this maximum, we will need functions for calculating the value, gradient (sum of scores), and Hessian of $\\mathcal{L}_N(\\gamma)$. We define these functions next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the sample log-likelihood\n",
    "\n",
    "We first define functions for calculating the individual and sample log-likelihoods. These are straightforward applications of the formulas above. In computing the sample log likelihood, however, we apply one special wrinkle in initializing the output vector which allows Julia to determine the type of the function output dynamically. This allows us to apply automatic differentiation methods to compute the gradient as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vector_log_likelihood (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute observation (it) level log likelihood: log(P_it) if d_it > 0, log(1-P_it) else\n",
    "function period_log_likelihood(gamma, d_it, w_it)\n",
    "    ccp = predicted_service_prob(gamma, w_it)\n",
    "    if d_it > 0\n",
    "        return log(ccp)\n",
    "    else\n",
    "        return log(1 - ccp)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Compute sample log likelihood: summing over periods\n",
    "function sample_log_likelihood(gamma, data)\n",
    "    \n",
    "    # Retrieve relevant columns of data frame in matrix form\n",
    "    D = data[!, :D]\n",
    "    W = convert(Matrix, data[:, Symbol.(:W, 1:3)])\n",
    "    \n",
    "    # Initialize log likelihood to first value\n",
    "    #  Note: we compute this separately to allow output type to be determined by Julia\n",
    "    #  As opposed, for example, to initializing sumll=0., which forces sumll to be a float\n",
    "    #  This doesn't matter for computing the numeric value of the log-likelihood\n",
    "    #  But it is required if we want to use the automatic differentiation methods employed below\n",
    "    sumll = period_log_likelihood(gamma, D[1], W[1,:])\n",
    "    \n",
    "    # Loop over remaining observations and compute overall log likelihood\n",
    "    for it=2:length(D)\n",
    "        sumll += period_log_likelihood(gamma, D[it], W[it, :])\n",
    "    end\n",
    "    \n",
    "    # Return sum log likelihood\n",
    "    return(sumll)\n",
    "end\n",
    "\n",
    "function vector_log_likelihood(gamma, data)\n",
    "    # Retrieve relevant columns of data frame in matrix form\n",
    "    D = data[!, :D]\n",
    "    W = convert(Matrix, data[:, Symbol.(:W, 1:3)])\n",
    "    delta = W*gamma\n",
    "    maxdelta = max.(0., delta)\n",
    "    expdelta0 = exp.(-maxdelta)\n",
    "    expdelta1 = exp.(delta .- maxdelta)\n",
    "    ccp = (D.*expdelta1 + (1. .- D).*expdelta0) ./ (expdelta0 + expdelta1)\n",
    "    return sum(log.(ccp))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next write a function to compute the value, gradient and Hessian of the log-likelihood function simultaneously. We use the ForwardDiff package in Julia to compute gradients and Hessians automatically -- a powerful tool based on the fact that all machine calculations ultimately boil down to addition, subtraction, multiplication and division (so the chain rule can be applied at the machine operation level). My implementation requires two additional packages, ForwardDiff and DiffResults, whose use is illustrated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_score_hessian (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ForwardDiff, DiffResults\n",
    "\n",
    "# Compute sample score and Hessian\n",
    "#   Here we use the ForwardDiff package, which allows efficient automatic computation of gradients and hessians\n",
    "function sample_score_hessian(gamma, data)\n",
    "    \n",
    "    # We first initialize a HessianResult structure that allows us to compute objective, score, and Hessian in one shot\n",
    "    #  Here the argument gamma describes the vector with which we aim to take derivatives\n",
    "    hessres = DiffResults.HessianResult(gamma)\n",
    "    \n",
    "    # We now apply the ForwardDiff.hessian! method to compute the gradient and Hessian of the log-likelihood\n",
    "    #  We first specify the log-likelihood as an anonymous function of gamma only\n",
    "    #  We then call ForwardDiff.hessian! to fill the results of hessres\n",
    "    #  Note the !, which specifies that this function will modify one of its arguments\n",
    "    #  In this case, it will modify the hessres structure, which will ultimately contain the outputs\n",
    "    func = g -> sample_log_likelihood(g, data)\n",
    "    ForwardDiff.hessian!(hessres, func, gamma)\n",
    "    \n",
    "    # Finally, we retrieve the objective, score and gradient from the HessRes structure\n",
    "    sumll = DiffResults.value(hessres)\n",
    "    sumscore = DiffResults.gradient(hessres)\n",
    "    sumhessian = DiffResults.hessian(hessres)\n",
    "    return(sumll, sumscore, sumhessian)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.861877 seconds (5.62 M allocations: 284.897 MiB, 4.94% gc time)\n",
      "  0.109122 seconds (412.11 k allocations: 20.283 MiB, 6.10% gc time)\n",
      "  0.010773 seconds (98.06 k allocations: 5.468 MiB)\n",
      "  0.009373 seconds (98.04 k allocations: 3.025 MiB)\n",
      "  0.009957 seconds (98.06 k allocations: 5.468 MiB)\n",
      "  0.009149 seconds (98.04 k allocations: 3.025 MiB)\n",
      "  0.013715 seconds (98.06 k allocations: 5.468 MiB, 29.19% gc time)\n",
      "  0.009261 seconds (98.04 k allocations: 3.025 MiB)\n",
      "  0.009542 seconds (98.06 k allocations: 5.468 MiB)\n",
      "  0.008768 seconds (98.04 k allocations: 3.025 MiB)\n",
      "  0.008960 seconds (98.06 k allocations: 5.468 MiB)\n",
      "sumll = -2928.4100729654224\n",
      "sumscore = [32.8619, -164.052, -9.69842]\n",
      "sumhess = [-889.615 3400.42 659.185; 3400.42 -14781.1 -2552.94; 659.185 -2552.94 -2552.94]\n"
     ]
    }
   ],
   "source": [
    "# Test the log-likelihood, score, and Hessian functions\n",
    "#   Note the large difference in time to run between the first call of sample_score_hessian and subsequent calls\n",
    "#   This is a function of Julia's just-in-time compiler, which must run once the first time the function is called\n",
    "#   Note further that (at least in this example) there's hardly any difference between the time required to \n",
    "#   evaluate the log-likelihood only, and the time required to additionally compute the score and Hessian\n",
    "@time sumll, sumscore, sumhess = sample_score_hessian(gamma0, data)\n",
    "for ii=1:5\n",
    "    gamma1 = gamma0 + .1*rand(length(gamma0))\n",
    "    @time sample_log_likelihood(gamma1, data)\n",
    "    @time sumll1, sumscore1, sumhess1 = sample_score_hessian(gamma1, data);\n",
    "end\n",
    "@show sumll;\n",
    "@show sumscore;\n",
    "@show sumhess;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also write a function to compute a matrix estimating the expected outer product of individual scores. Recall that $x_{it}$'s are only independent across $i$, not across $t$. Hence, formally speaking, we want an estimator of\n",
    "$$\n",
    "    B_0 = E[s_{i}(\\gamma)s_{i}(\\gamma)'], \n",
    "$$\n",
    "where $s_i(\\gamma)$ denotes the sum of scores for individual $i$:\n",
    "$$\n",
    "    s_i(\\gamma) = \\sum_{t=1}^{T} s_{it}(\\gamma).\n",
    "$$\n",
    "In other words, formally speaking, we want the average of the outer products of individual-level scores $s_{i}(\\gamma)$ across individuals $i$, rather than the average of the outer products of the individual-period scores $s_{it}(\\gamma)$ across both $i$ and $t$. However, since the model satisfies dynamic completeness, one can show (see Wooldridge 13.8.3) that all interactions of cross-time scores have mean zero, from which it follows that\n",
    "$$\n",
    "    E[s_{i}(\\gamma_0)s_{i}(\\gamma_0)'] = E\\left[\\sum_{t=1}^T s_{it}(\\gamma_0) s_{it}(\\gamma_0)'\\right]. \n",
    "$$\n",
    "In other words, we may estimate $B_0$ by \n",
    "$$\n",
    "    \\hat{B} = \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T s_{it}(\\gamma_0) s_{it}(\\gamma_0)',\n",
    "$$\n",
    "i.e., by taking the sum of the outer product of observation-level scores $s_{it}(\\gamma_0)$ across observations $i$ and $t$, then normalizing by the number of individuals $N$. \n",
    "\n",
    "In practice, we simply compute a non-normalized sum of outer products; we normalize later. In other words, we effectively compute $N\\cdot \\hat{B}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.430300 seconds (1.59 M allocations: 83.434 MiB, 5.93% gc time)\n"
     ]
    }
   ],
   "source": [
    "function get_sum_outer_prod_scores(gamma, data)\n",
    "    \n",
    "    # Retrieve data in matrix form\n",
    "    D = data[!, :D]\n",
    "    W = convert(Matrix, data[:, Symbol.(:W, 1:3)])\n",
    "    \n",
    "    # Compute period-level scores using ForwardDiff\n",
    "    sum_outer_prod_score = zeros(length(gamma), length(gamma))\n",
    "    for it=1:length(D)\n",
    "        func = g -> period_log_likelihood(g, D[it], W[it,:])\n",
    "        score_it = ForwardDiff.gradient(func, gamma)\n",
    "        sum_outer_prod_score += score_it*score_it'\n",
    "    end\n",
    "    return(sum_outer_prod_score)\n",
    "end\n",
    "\n",
    "@time sumouterprodscore = get_sum_outer_prod_scores(gamma0, data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.015355 seconds (109.07 k allocations: 9.297 MiB, 38.52% gc time)\n",
      "  0.009266 seconds (109.07 k allocations: 9.297 MiB)\n",
      "  0.013832 seconds (109.07 k allocations: 9.297 MiB, 32.43% gc time)\n",
      "  0.009284 seconds (109.07 k allocations: 9.297 MiB)\n",
      "  0.009272 seconds (109.07 k allocations: 9.297 MiB)\n",
      "  0.012864 seconds (109.07 k allocations: 9.297 MiB, 27.62% gc time)\n",
      "  0.009746 seconds (109.07 k allocations: 9.297 MiB)\n",
      "  0.013246 seconds (109.07 k allocations: 9.297 MiB, 28.29% gc time)\n",
      "  0.009242 seconds (109.07 k allocations: 9.297 MiB)\n",
      "  0.012961 seconds (109.07 k allocations: 9.297 MiB, 27.50% gc time)\n"
     ]
    }
   ],
   "source": [
    "# Out of curiosity, let's compare the execution time of outer prod scores to the full Hessian\n",
    "# In this example, apart from the initial compile time, there doesn't appear to be much difference!\n",
    "for ii=1:10\n",
    "    gamma1 = gamma0 + .1*rand(length(gamma0))\n",
    "    @time sumouterprodscore = get_sum_outer_prod_scores(gamma1, data)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just for fun, let's verify the information matrix equality. Using the functions above, we estimate both $\\hat{A}$ and $\\hat{B}$, and compute the elementwise percentage difference between $\\hat{A}$ and $\\hat{B}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " -0.00271418  -0.0133028  -0.00481647\n",
       " -0.0133028   -0.0229311  -0.0279216 \n",
       " -0.00481647  -0.0279216  -0.0279216 "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check information matrix equality elementwise \n",
    "N = size(data, 1)\n",
    "sumll, sumscore, sumhess = sample_score_hessian(gamma0, data)\n",
    "Ahat = -sumhess / N\n",
    "Bhat = get_sum_outer_prod_scores(gamma0, data) / N\n",
    "(Ahat - Bhat) ./ Ahat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the maximization problem\n",
    "\n",
    "We now have functions to compute the log-likelihood, score, and Hessian of the MLE problem. Let $\\mathcal{S}_N(\\gamma)$ and $\\mathcal{H}_N(\\gamma)$, respectively, denote the sum of scores and sum of Hessians in the sample at parameter value $\\gamma$. We aim to find the parameter $\\hat{\\gamma}$ solving $\\max_{\\gamma} \\mathcal{L}_N(\\gamma)$. \n",
    "\n",
    "Toward this end, we will use Newton-Raphson type iteration to solve the maximization problem. For illustrative purposes, we will code the maximization algorithms ourselves. Julia, like Matlab, also has pre-coded maximization functions; we'll compare our solution to these below. \n",
    "\n",
    "Newton-Raphson type algorithms are based on the following iteration strategy. We know the optimum $\\hat{\\gamma}$ satisfies \n",
    "$$\n",
    "\\mathcal{S}_N(\\hat{\\gamma}) = 0. \n",
    "$$\n",
    "Let $\\gamma^k$ be some guess for the maximizer $\\hat{\\gamma}$. We seek to find a next step $\\gamma^{k+1}$ such that the score function $\\mathcal{S}_N(\\gamma^{k+1})$ is as close as possible to zero. Toward this end, consider the Taylor expansion of $\\mathcal{S}_N(\\gamma)$ around our current guess $\\gamma^k$: \n",
    "$$\n",
    "\\mathcal{S}_N(\\gamma^{k+1}) \\approx \\mathcal{S}_N(\\gamma^{k}) + \\mathcal{H}_N(\\gamma^{k})(\\gamma^{k+1} - \n",
    "\\gamma^k).\n",
    "$$\n",
    "Bearing this approximation in mind, we therefore choose the next step $\\gamma^{k+1}$ to make the right-hand side of the Taylor approximation zero, which (we hope) implies $\\mathcal{S}_N(\\gamma^{k+1})$ close to zero. Solving for $\\gamma^{k+1}$ from the resulting expression, we ultimately obtain:\n",
    "$$\n",
    "\\gamma^{k+1} = \\gamma^k + \\left[-\\mathcal{H}_N(\\gamma^{k})\\right]^{-1} \\mathcal{S}_N(\\gamma^{k}).\n",
    "$$\n",
    "For any initial guess $\\gamma^{0}$ \"close enough\" to $\\hat{\\gamma}$, this iterative scheme will yield a sequence $\\gamma^{k}$ converging to the optimum $\\hat{\\gamma}$ at a *quadratic* rate: that is, the error in step $k+1$ will decrease with the square of the step-$k$ error (which is very fast when the current error is small!). Unfortunately, for \"bad\" initial guesses (i.e., those outside an a priori unknown neighborhood of convergence), the pure NR algorithm is not guaranteed to converge at all -- especially when the objective function is not globally concave. Furthermore, in some cases (though not here), the Hessian can be costly to code and / or evaluate. \n",
    "\n",
    "For these reasons, in practice, numerical optimization procedures typically employ two modifications to the baseline NR algorithm. First, in place of the exact Hessian $\\mathcal{H}_N(\\gamma^{k})$, it is common to employ another matrix $H^k$ --- typically an approximation of the Hessian which is either (a) easier to compute or (b) guaranteed to be negative definite. This yields a baseline step $\\Delta^k$ given by\n",
    "$$\n",
    "\\Delta^k = [-H^k]^{-1} \\mathcal{S}_N(\\gamma^{k}).\n",
    "$$\n",
    "Second, rather than take the baseline step $\\Delta^k$ directly, it is typical to scale the actual step size by a constant $\\alpha^k$, so that the actual iteration is \n",
    "$$\n",
    "    \\gamma^{k+1} = \\gamma^k + \\alpha^k \\Delta^k.\n",
    "$$\n",
    "In practice, the step length $\\alpha^k$ is typically determined by a line search along the direction $\\Delta^k$ for a suitable improvement in the objective function $\\mathcal{L}_N(\\gamma^k + \\alpha \\Delta^k)$. We implement a very simplistic version of this line search (stop when any improvement found) below.\n",
    "\n",
    "Theoretically, such Newton-Raphson type algorithms have the following convergence properties:\n",
    "\n",
    "- So long as the search direction $\\Delta^k$ satisfies $\\mathcal{S}_N(\\gamma^{k})' \\cdot \\Delta^k > 0$, there will exist $\\alpha^k > 0$ such that $\\mathcal{L}_N(\\gamma^k + \\alpha^k \\Delta^k) \\geq \\mathcal{L}_N(\\gamma^k)$. In other words, so long as the search direction $\\Delta^k$ is in the same half-space as the gradient $\\mathcal{S}_N(\\gamma^{k})$, sufficiently small steps along the search direction will improve the objective function. Observe that since $\\mathcal{S}_N(\\gamma^{k})' \\Delta^k = [-H^k]^{-1} \\mathcal{S}_N(\\gamma^{k})$, we can guarantee $\\mathcal{S}_N(\\gamma^{k})' \\Delta^k > 0$ by ensuring that the Hessian approximator $H^k$ is negative definite ate each iteration $k$. \n",
    "\n",
    "- So long as at each iteration $k$ the step length $\\alpha^k$ is chosen such that the step $\\alpha^k \\Delta^k$ leads to an improvement in the objective function, any NR-type algorithm will eventually converge to a local maximum. \n",
    "\n",
    "In practice, there are many ways to choose the Hessian approximator $H^k$. Some leading examples are:\n",
    "\n",
    "1. The exact Hessian, potentially adjusted to ensure negative definiteness if necessary: e.g. $H^k = \\mathcal{H}_N(\\gamma^{k}) - \\lambda^k I$, where $\\lambda^k \\geq 0$ is chosen to ensure negative definiteneness (e.g., $\\lambda^k = 0$ if $\\mathcal{H}_N(\\gamma^{k})$ is already negative definite, otherwise $\\lambda^k$ strictly exceeds the maximum positive eigenvalue of $\\mathcal{H}_N(\\gamma^{k})$). \n",
    "\n",
    "2. For MLE models, the Berndt–Hall–Hall–Hausman (BHHH) step is available, which defines $-H^k$ as the sum of outer products of scores computed above. This substitution is justified by the information matrix equality, which tells us that close to the optimum the negative expected Hessian $-A_0$ should equal the expected score variance $B_0$. For MLE models, the BHHH step has two practical advantages. First, it only requires us to evaluate scores, not Hessians. Second, since the outer product of scores is always positive definite, the BHHH approximation ensures that we can always find an improvement in the objective function along the BHHH step direction.\n",
    "\n",
    "3. The Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm, which constructs an iterative approximation $H^k$ to the actual Hessian $\\mathcal{H}_N(\\gamma^{k})$ using only information on the score $\\mathcal{S}_N(\\gamma^{k})$ at each step, in such a way that $-H^k$ is always positive definite. Computationally, the BFGS algorithm is attractive in settings where the exact Hessian $\\mathcal{H}_N(\\gamma^{k})$ is difficult to compute, since BFGS requires only information on the score (which we need to compute anyway) to construct the approximation $H^k$. Furthermore, as with BHHH, BFGS ensures that we can always find an improvement along the search direction. \n",
    "\n",
    "For this problem, we have already computed both the Hessian and outer product of scores, so both the exact Hessian and BHHH approaches are available. We explore both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.408047 seconds (5.05 M allocations: 242.638 MiB, 6.89% gc time)\n",
      "llhat = -2927.337751836076\n",
      "scorehat = [5.44192e-12, 4.9174e-12, 2.44249e-14]\n"
     ]
    }
   ],
   "source": [
    "# Solve MLE problem using hybrid of pure Hessian and BHHH approaches\n",
    "function solve_mle_problem(gamma0, data, usebhhh=false)\n",
    "    maxit = 1000\n",
    "    ll0, score0, hess0 = sample_score_hessian(gamma0, data)\n",
    "    for ii=1:maxit\n",
    "        \n",
    "        # If useBHHH option is flagged: use BHHH hessian step\n",
    "        if usebhhh\n",
    "            hess0 = -get_sum_outer_prod_scores(gamma0, data)\n",
    "\n",
    "        # Else check whether Hessian is negative definite; if not, fall back on BHHH\n",
    "        elseif !isposdef(-hess0) \n",
    "            hess0 = -get_sum_outer_prod_scores(gamma0, data)\n",
    "            print(\"Hessian is not negative definite! Falling back on BHHH.\\n\")\n",
    "        end\n",
    "        \n",
    "        # Compute initial NR-type step\n",
    "        #   Note: hess \\ score (i.e. solve H step = score)) is both more stable and more efficient \n",
    "        #   than inv(hess)*score.\n",
    "        step = -hess0 \\ score0\n",
    "        \n",
    "        # If initial step length > 1: normalize to max length 1 (prevent crazy steps)\n",
    "        stepnorm = max(1., norm(step))\n",
    "        step ./ stepnorm\n",
    "        \n",
    "        # Cut back step until log-likelhood improvement found (or 10 iterations, whichever happens first)\n",
    "        gamma1 = gamma0 + step\n",
    "        ll1 = sample_log_likelihood(gamma1, data)\n",
    "        for ctr = 1:10\n",
    "            if (ll1 > ll0)\n",
    "                break\n",
    "            end\n",
    "            step ./ 2\n",
    "            gamma1 = gamma0 + step\n",
    "            ll1 = sample_log_likelihood(gamma1, data)\n",
    "        end\n",
    "        \n",
    "        # Update current values \n",
    "        gamma0 = gamma1\n",
    "        ll0, score0, hess0 = sample_score_hessian(gamma0, data)\n",
    "        \n",
    "        # Display status every 10 iterations\n",
    "        if (ii % 10) == 0\n",
    "            @show ii, ll0, score0\n",
    "        end\n",
    "        \n",
    "        # Check for convergence: norm of score less than 1e-6\n",
    "        if norm(score0) < 1e-6\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Return results\n",
    "    return(gamma0, ll0, score0, hess0)\n",
    "end\n",
    "\n",
    "# Find MLE solution from the true parameters\n",
    "@time gammahat, llhat, scorehat, hesshat = solve_mle_problem(gamma0, data)\n",
    "@show llhat;\n",
    "@show scorehat;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "  0.268562 seconds (2.52 M allocations: 137.716 MiB, 8.36% gc time)\n",
      "  0.442749 seconds (4.01 M allocations: 205.424 MiB, 7.61% gc time)\n",
      "gammahat = [4.96094, 0.9787, 0.207439]\n",
      "llhat = -2927.337751836066\n"
     ]
    }
   ],
   "source": [
    "# Verify MLE solution from different starting point\n",
    "gamma1 = gamma0*0.\n",
    "\n",
    "# Solve baseline algorithm using exact Hessian where possible\n",
    "@time gammahat, llhat, scorehat, hesshat = solve_mle_problem(gamma1, data, false)\n",
    "\n",
    "# Solve using BHHH approximation at each step\n",
    "@time gammahat, llhat, scorehat, hesshat = solve_mle_problem(gamma1, data, true)\n",
    "\n",
    "# Display results\n",
    "@show gammahat; \n",
    "@show llhat;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Inference\n",
    "\n",
    "Finally, we compute asymptotic MLE confidence intervals. We consider both the $\\hat{A}$ and $\\hat{B}$ versions, and compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " 0.00938617    0.00209045    0.000328924\n",
       " 0.00209045    0.000546721  -7.88583e-6 \n",
       " 0.000328924  -7.88583e-6    0.000481501"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asymptotic VCV based on Hessian matrix\n",
    "hesshat = sample_score_hessian(gammahat, data)[3]\n",
    "VCVh = -hesshat \\ I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " 0.00953737    0.00212494    0.000282597\n",
       " 0.00212494    0.000553566  -1.71634e-5 \n",
       " 0.000282597  -1.71634e-5    0.000469539"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asymptotic VCV based on score variance matrix\n",
    "vshat = get_sum_outer_prod_scores(gammahat, data)\n",
    "VCVvs = vshat \\ I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Coef</th><th>Est</th><th>se_h</th><th>se_vs</th><th>tstats</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>3 rows × 5 columns</p><tr><th>1</th><td>gamma0</td><td>4.96094</td><td>0.0968823</td><td>0.0976595</td><td>-0.403166</td></tr><tr><th>2</th><td>gamma1</td><td>0.9787</td><td>0.0233821</td><td>0.023528</td><td>-0.910976</td></tr><tr><th>3</th><td>gamma2</td><td>0.207439</td><td>0.0219431</td><td>0.0216689</td><td>0.339006</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& Coef & Est & se\\_h & se\\_vs & tstats\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & gamma0 & 4.96094 & 0.0968823 & 0.0976595 & -0.403166 \\\\\n",
       "\t2 & gamma1 & 0.9787 & 0.0233821 & 0.023528 & -0.910976 \\\\\n",
       "\t3 & gamma2 & 0.207439 & 0.0219431 & 0.0216689 & 0.339006 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "3×5 DataFrame\n",
       "│ Row │ Coef   │ Est      │ se_h      │ se_vs     │ tstats    │\n",
       "│     │ \u001b[90mString\u001b[39m │ \u001b[90mFloat64\u001b[39m  │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │\n",
       "├─────┼────────┼──────────┼───────────┼───────────┼───────────┤\n",
       "│ 1   │ gamma0 │ 4.96094  │ 0.0968823 │ 0.0976595 │ -0.403166 │\n",
       "│ 2   │ gamma1 │ 0.9787   │ 0.0233821 │ 0.023528  │ -0.910976 │\n",
       "│ 3   │ gamma2 │ 0.207439 │ 0.0219431 │ 0.0216689 │ 0.339006  │"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coef + SE table\n",
    "gamma_mle = gammahat\n",
    "coef_names = [\"gamma0\"; \"gamma1\"; \"gamma2\"]\n",
    "se_h = sqrt.(diag(VCVh))\n",
    "se_vs = sqrt.(diag(VCVvs))\n",
    "tstats = (gamma_mle - gamma0) ./ se_h\n",
    "results = DataFrame(:Coef => coef_names, :Est => gammahat, :se_h => se_h, :se_vs => se_vs, :tstats => tstats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-val of H0: gamma = gamma0: 0.45706574343795947"
     ]
    }
   ],
   "source": [
    "# Do we reject true gamma? \n",
    "ll0 = sample_log_likelihood(gamma0, data)\n",
    "LR = 2*(llhat - ll0)\n",
    "pval = cdf(Chisq(3), LR)\n",
    "print(\"P-val of H0: gamma = gamma0: $(pval)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Estimation\n",
    "\n",
    "Now let's consider GMM estimation. By definition, the predicted choice probability function $P(w_{it}, \\gamma_0)$ is also the conditional mean of the binary variable $d_{it}$ conditional on the covariates $w_{it}$. Hence, for all $w_{it}$, we obtain the *conditional moment restriction*\n",
    "$$\n",
    "E[d_{it} - P(w_{it}, \\gamma_0)|w_{it}].\n",
    "$$\n",
    "To apply GMM, we need to translate this conditional moment restriction into a vector of unconditional moment restrictions. Toward this end, let $r_{it}(\\gamma) = d_{it} - P(w_{it}, \\gamma)$ denote the residual function implied by our model, and observe that\n",
    "$$\n",
    "E[F(w_{it}) r_{it}(\\gamma_0)] = 0 \\quad \\mbox{for all functions } F(w_{it}).\n",
    "$$\n",
    "In other words, any choice of instruments $Z_{it} = F(w_{it})$ will produce valid moment restrictions if interacted with the residual $r_{it}(\\gamma_0)$. We proceed to discuss several potential choices for the instruments $Z_{it}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of Moments with efficient instruments\n",
    "\n",
    "The theoretically efficient choice for the instruments $Z_{it}$ is\n",
    "$$\n",
    "Z_{it} = \\Omega(w_{it})^{-1} R_{it}(\\gamma_0),\n",
    "$$\n",
    "where $\\Omega(w_{it}) = Var(r_{it}(\\gamma_0) | w_{it})$ is the variance of the true residual $r_{it}(\\gamma_0)$ conditional on $w_{it}$, and $R_{it}(\\gamma) = \\nabla_\\gamma r_{it}(\\gamma)$ is the gradient of this residual with respect to $\\gamma$. \n",
    "\n",
    "It turns out that the logit model is one case when we can in fact easily derive the efficient instruments. By hypothesis, $d_{it}$ is binomially distributed conditional on $w_{it}$ with mean $P_{it}$. It follows that the conditional variance of $r_{it}$ given $w_{it}$ is\n",
    "$$\n",
    "\\Omega(w_{it}) = P_{it} (1 - P_{it}). \n",
    "$$\n",
    "Meanwhile, with a little algebra one can show that for the logit model, the gradient of $P_{it}$ can be written as\n",
    "$$\n",
    "    \\nabla_{\\gamma} P_{it} = P_{it} (1-P_{it})w_{it}.\n",
    "$$\n",
    "It follows that the efficient instruments are\n",
    "$$\n",
    "    Z_{it} = \\Omega(w_{it})^{-1} R_{it}(\\gamma_0) = \\frac{ P_{it} (1-P_{it})w_{it}}{P_{it} (1-P_{it})} = w_{it}. \n",
    "$$\n",
    "In other words, for the logit model, the naive choice $Z_{it} = w_{it}$ is in fact efficient!\n",
    "\n",
    "(This is a special feature of the logit model; it would not hold, for example, in a probit model. It is driven by the trademark logit gradient form noted above, which we'll see repeatedly throughout the course.)\n",
    "\n",
    "Forming orthogonality conditions based on the efficient instruments $Z_{it} = w_{it}$, we obtain the $3 \\times 1$ vector of moment restrictions\n",
    "$$\n",
    "E[w_{it} r_{it}(\\gamma)] = 0.\n",
    "$$\n",
    "Since the number of moments is exactly the same as the number of parameteres, this choice of instruments produces an exactly identified model to which the simple Method of Moments can be applied. \n",
    "\n",
    "To implement estimation based on this choice, we first define our moment function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mom_moment_jacobian (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Moment vector based on efficient instruments Z_{it} = w_{it}\n",
    "function mom_moment_vector(gamma, data)\n",
    "    \n",
    "    # Retrieve relevant columns of data frame in matrix form\n",
    "    D = data[!, :D]\n",
    "    W = convert(Matrix, data[:, Symbol.(:W, 1:3)])\n",
    "    \n",
    "    # reset moment vector\n",
    "    mv = zeros(typeof(gamma[1]), 3)\n",
    "    \n",
    "    # Compute sample moments \n",
    "    for it=1:length(D)\n",
    "        w_it = W[it, :]\n",
    "        p_it = predicted_service_prob(gamma, w_it)\n",
    "        mv += (D[it] - p_it) .* w_it\n",
    "    end\n",
    "    return(mv)\n",
    "end\n",
    "\n",
    "# Compute Jacobian using automatic differentiation\n",
    "function mom_moment_jacobian(gamma, data)\n",
    "    func = g -> mom_moment_vector(g, data)\n",
    "    jac = ForwardDiff.jacobian(func, gamma)\n",
    "    return jac \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than writing our own solver as above, we here use the NLsolve package to solve the nonlinear moment system for the MOM estimator $\\hat{\\gamma}$. This requires a few small transformations to ensure that our functions are in the form expected by NLsolve, which we wrap within the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "find_mom_estimate (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using NLsolve\n",
    "\n",
    "function find_mom_estimate(gamma0, data)\n",
    "   \n",
    "    # Write moment function in form expected by NLsolve (returning moment values F in place)\n",
    "    function mom!(F, g)\n",
    "        F[:] = mom_moment_vector(g, data)\n",
    "    end\n",
    "    \n",
    "    # Write Jacobian function in form expected by NLsolve (returning Jacobian values J in place)\n",
    "    function jac!(J, g)\n",
    "        J[:] = mom_moment_jacobian(g, data)\n",
    "    end\n",
    "    \n",
    "    # Call NLsolve optimizer\n",
    "    res = nlsolve(mom!, jac!, gamma0)\n",
    "    return res\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma_mom = [4.96094, 0.9787, 0.207439]\n",
      "gamma_mle = [4.96094, 0.9787, 0.207439]\n",
      "gamma_mom - gamma_mle = [-1.41374e-10, -2.89451e-11, -2.10844e-10]\n"
     ]
    }
   ],
   "source": [
    "# Run MOM estimator from initial guess gamma = [0,0,0]\n",
    "res = find_mom_estimate([0.; 0.; 0.], data)\n",
    "gamma_mom = res.zero\n",
    "@show gamma_mom; \n",
    "@show gamma_mle; \n",
    "@show gamma_mom - gamma_mle;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our MOM estimator is in fact *identical* to the MLE estimator! I did not anticipate this! (Although I probably should have.)\n",
    "\n",
    "However, it turns out that, after some algebra, one can show that in the logit model, the MLE sum of scores is indeed mathematically equivalent to the sample moments based on the instrument choice $Z_{it} = w_{it}$. This is a special feature of the logit model; it would not hold, for example, in a probit model. This is driven by the fact that for the logit model, the gradient of $P_{it}$ can be written as\n",
    "$$\n",
    "    \\nabla_{\\gamma} P_{it} = P_{it} (1-P_{it})w_it,\n",
    "$$\n",
    "This turns out to drive several simplifications which ultimately reduce the MLE sample score to the MOM moment vector. \n",
    "\n",
    "Given that MOM based on the efficient choice of instruments $w_{it}$ leads to a moment vector which is equivalent to the sample score, the asymptotic properties of this particular MOM estimator will be equivalent to MLE. \n",
    "\n",
    "Note that we can *always* reframe MLE as MOM based on the restriction that the sample score has zero mean: \n",
    "$$\n",
    "    E[s_{it}(\\gamma_0)] = 0.\n",
    "$$\n",
    "The unique feature of this example is that the particular choice of instruments $Z_{it} = w_{it}$ turns out to reproduce the sample score exactly. This is special feature of the logit model, and typically won't hold true more generally!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM based on other potential interactions\n",
    "\n",
    "Given that we have already found the theoretically optimal instrument vector, there is in this example little reason to go on to consider general GMM. For completeness, however, we proceed to do so. \n",
    "\n",
    "Specifically, suppose that we add the square of mileage since last service to our instrument set (motivated, perhaps, by concerns over potential nonlinearity in the choice probability in this variable). That is, we consider the instrument vector $Z_{it} = [w_{it}, z_{it}^2]$, rather than simply $Z_{it} = w_{it}$ as above. We then base estimation on the moment restrictions\n",
    "$$\n",
    "E[Z_{it} r_{it}(\\gamma)] = 0.\n",
    "$$\n",
    "\n",
    "We now have $L=4$ moment restrictions in $P=3$ parameters, so $L > P$ and the model is overidentified. We therefore need to use full GMM. \n",
    "\n",
    "We begin by writing a function which performs GMM estimation for any choice of weighting matrix $W$. We then construct the optimal weighting matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run_gmm_estimation (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Optim \n",
    "\n",
    "# Compute (non-normalized) GMM moment vector\n",
    "function gmm_moment_vector(gamma, data)\n",
    "    D = data[!, :D]\n",
    "    W = convert(Matrix, data[:, Symbol.(:W, 1:3)])\n",
    "    Z = [W  data[!, :Z].^2]\n",
    "    \n",
    "    # reset moment vector\n",
    "    mv = zeros(typeof(gamma[1]), 4)\n",
    "    \n",
    "    # Compute sample moments \n",
    "    for it=1:length(D)\n",
    "        p_it = predicted_service_prob(gamma, W[it, :])\n",
    "        mv += (D[it] - p_it) .* Z[it, :]\n",
    "    end\n",
    "    return(mv)\n",
    "end\n",
    "\n",
    "# Compute (non-normalized) GMM objective\n",
    "function gmm_objective(gamma, data, W)\n",
    "    mv = gmm_moment_vector(gamma, data)\n",
    "    return(mv'*W*mv)\n",
    "end\n",
    "\n",
    "# Find GMM estimator starting from initial guess gamma0\n",
    "function run_gmm_estimation(gamma0, data, W)\n",
    "    gmmobj = g -> gmm_objective(g, data, W)\n",
    "    res = optimize(gmmobj, gamma0, BFGS(); autodiff = :forward)\n",
    "    return res\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-step GMM based on 2SLS weighting matrix\n",
    "\n",
    "Since we form moments by interactions with instruments, we start with the 2SLS weighting matrix \n",
    "$$\n",
    "    W_1 = \\left(\\sum Z_{it}Z_{it}' \\right)^{-1}. \n",
    "$$\n",
    "We derive a first-step estimate $\\hat{\\gamma}_1$ based on this first-step weighting matrix, estimate the efficient weighting matrix $W_2$, then obtain an efficient GMM estimate $\\hat{\\gamma}_1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " -0.04857172595510573  \n",
       " -0.011064235598256311 \n",
       " -0.0014261833308610028"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First step: 2SLS weighting matrix\n",
    "Z = [convert(Matrix, data[:, Symbol.(:W, 1:3)])  data[!, :Z].^2]\n",
    "W0 = Z'*Z / length(Z)\n",
    "res = run_gmm_estimation(gamma0, data, W0)\n",
    "gamma_gmm = res.minimizer\n",
    "@show gamma_gmm - gamma_mle; \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second-step GMM based on efficient weighting matrix\n",
    "\n",
    "We next proceed to estimate the efficient weighting matrix\n",
    "$$\n",
    "    W_2 = \\sum_{i=1}^{N} \\sum_{t=1}^{T} Z_{it} r_{it}(\\hat\\gamma_1).\n",
    "$$\n",
    "Note that dynamic completeness again implies that the cross-correlation of moment components $Z_{it} r_{it}(\\gamma_0)$ is zero across distinct periods, so it is fine to focus on a simple sum over $i$ and $t$ in estimating the weighting matrix, although we conduct inference asymptotically only as $N \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute (non-normalized) GMM moment variance \n",
    "function gmm_moment_variance(gamma, data)\n",
    "    D = data[!, :D]\n",
    "    W = convert(Matrix, data[:, Symbol.(:W, 1:3)])\n",
    "    Z = [W  data[!, :Z].^2]\n",
    "    \n",
    "    # reset moment vector\n",
    "    mvar = zeros(typeof(gamma[1]), 4, 4)\n",
    "    \n",
    "    # Compute variance of sample moments across i \n",
    "    for it=1:length(D)\n",
    "        p_it = predicted_service_prob(gamma, W[it, :])\n",
    "        mv = (D[it] - p_it) .* Z[it, :]\n",
    "        mvar += mv * mv'\n",
    "    end\n",
    "    return(mvar)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
