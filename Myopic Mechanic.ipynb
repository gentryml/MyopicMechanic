{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Myopic Mechanic\n",
    "\n",
    "Consider a car owner (let's call him \"Mitt\") facing the problem of when to take his car to the mechanic for service. We assume that Mitt makes one vehicle service decision per month, at the beginning of each month.\n",
    "\n",
    "Let $x$ denote the total miles on the car, and $z$ denote the number of miles since the last service visit (both measured in thousands). Conditional on the total mileage $x$ and the miles since last service $z$, Mitt perceives expected beginning-of-month benefit from operating the car given by\n",
    "$$\n",
    "U(x, z) = a_0 + a_1 x - \\rho_1 z - \\rho_2 \\cdot z \\mathbb{I}[x \\geq 100],\n",
    "$$\n",
    "where $\\rho_1 > 0$ and $\\rho_2 >0 $ represent expected current costs associated with increasing miles since last service visit, including both routine costs and potential breakdown risks, and $\\mathbb{I}[x \\geq 100]$ is an indicator for whether the car has more than 100,000 miles. Meanwhile, visiting the mechanic involves average cost \n",
    "$$\n",
    "C(x, z) = c_0 + c_1 z + c_2 \\cdot z \\mathbb{I}[x \\geq 100],\n",
    "$$\n",
    "where we allow costs of the service visit to increase with miles since last service, but assume that $\\rho_1 > c_1$ and $\\rho_2 > c_2$ (so that as $z$ increases, perceived costs of inaction increase faster than perceived costs of service).\n",
    "\n",
    "If Mitt visits the mechanic, he will incur an expected cost $C(x, z)$ specified above. However, visiting the mechanic resets the number of miles $z$ since the last service visit to zero, allowing Mitt to realize benefit $U(x,0)$ over the rest of the month. \n",
    "\n",
    "At the beginning of every month, Mitt decides whether or not to take his car to the mechanic. \n",
    "Toward this end, he compares the net monthly benefit of taking the car to the mechanic $(d=1)$, \n",
    "\\begin{equation}\n",
    "V_1 = U(x,0) - C(x, z) = a_0 + a_1 x - c_0 - c_1 z - c_2 \\cdot z \\mathbb{I}[x \\geq 100] + \\epsilon_1,\n",
    "\\end{equation}\n",
    "to the  net monthly benefit of operating the car without maintenance ($d=0$),\n",
    "$$\n",
    "V_0 = U(x, z) = a_0 + a_1 x - \\rho_1 z - \\rho_2 \\cdot z \\mathbb{I}[x \\geq 100] + \\epsilon_0,\n",
    "$$\n",
    "where $\\epsilon_0$ and $\\epsilon_1$ are i.i.d. Type 1 Extreme Value utility shocks representing idiosyncratic variation in Mitt's monthly tastes for mechanic visits (much more on these soon!). If $V_1 > V_0$, Mitt takes the car in ($d=1$); otherwise, he doesn't ($d=0$).\n",
    "\n",
    "Note that Mitt is *myopic*, in the sense that he considers only costs and benefits of maintenance within the current month, not prospective benefits in future months from maintaining the car today. We will return to the case of a forward-looking mechanic (Harold Zurcher, the subject of a seminal 1987 paper by John Rust) when we introduce dynamic discrete choice analysis.\n",
    "\n",
    "## Predicted probability of service\n",
    "\n",
    "We first compute the predicted probability that Mitt takes the car for service $(d=1)$ given $x$ and $z$: denote this conditional choice probability by $P(x,z)$. By definition, this probability is given by\n",
    "\\begin{align}\n",
    "P(x, z) &= P(V_1 - V_0 \\geq 0|x, z) \\\\\n",
    "&= P(-c_0 + (\\rho_1 - c_1)z + (\\rho_2 - c_2) z \\mathbb{I} [x \\geq 100k] \\geq \\epsilon_0 - \\epsilon_1) \\\\\n",
    "&= \\frac{\\exp(-c_0 + (\\rho_1 - c_1) z + (\\rho_2 - c_2) \\cdot z \\mathbb{I} [x \\geq 100])}\n",
    "{1 + \\exp(-c_0 + (\\rho_1 - c_1) z + (\\rho_2 - c_2) \\cdot z \\mathbb{I} [x \\geq 100])},\n",
    "\\end{align}\n",
    "where the last line follows by the i.i.d. Type I extreme value assumption on the idiosyncratic utility shocks $\\epsilon_0, \\epsilon_1$ (again, much more on this soon!). \n",
    "\n",
    "Note the following features of the predicted probability-of-service function $P(x,z)$:\n",
    "\n",
    "- The terms $a_0 + a_1x$ which appear in both $V_1$ and $V_0$ disappear from the choice probability function. We thus cannot identify either $a_0$ or $a_1$, since these do not affect the choice we observe. In other words, we cannot identify the effects of mileage per se on utility from driving the car; we can only identify effects of mileage insofar as they affect the relative costs of service.\n",
    "\t\n",
    "- Only differences $(\\rho_1 - c_1)$ and $(\\rho_2 - c_2)$ show up in the choice probability function. In this particular example, we can thus only identify the effect of mileage since last service on the differential costs of service versus not.\n",
    "\t\n",
    "- Since the scale of utility is arbitrary, we can identify these differences only up to scale. In assuming that $\\epsilon_0$ and $\\epsilon_1$ were i.i.d. Type 1 EV, we implicitly imposed a normalization on the variances of $\\epsilon_0$ and $\\epsilon_1$. Scaling all terms in utility by any positive constant would lead to the same choice probabilities.\n",
    "\t\n",
    "- Are these identified objects enough? It depends on the counterfactual of interest. For example, if we want to determine how cutting the base cost $c_0$ of a service visit in half would affect the frequency of service, we could do so. If we wanted to determine how a dollar subsidy to service would affect frequency of service, we could not, since in this simple exercise we have no way to convert estimated utilities into dollar terms. For this, we would need to observe variation in price of service, from which we abstract for the moment (as price raises a separate set of endogeneity questions which we will address in detail later). \n",
    "\n",
    "Bearing the above caveats in mind, redefine $\\gamma_0=c_0$, $\\gamma_1 = \\rho_1 - c_1$, and $\\gamma_2 = \\rho_2 - c_2$. These are the primitives which data on Mitt's service choices can identify. \n",
    "\n",
    "## Objectives of the exercise\n",
    "\n",
    "Suppose we observe panel data $(d_{it}, x_{it}, z_{it})_{t=1}^T$ on monthly mileage and service decisions for a collection of individuals $i=1,...,N$, interpreted as a random sample of the population. For simplicity, assume a balanced panel (i.e., the same $T$ for all $i$), although this is inessential.\n",
    "\n",
    "We aim to estimate the parameter vector $\\gamma = (\\gamma_0, \\gamma_1, \\gamma_2)$, assuming that each individual is making auto maintenance choices according to the model described above. Toward this end, we will consider:\n",
    "\n",
    "1. CMLE estimation based on the predicted choice probability function $P(x_{it}, z_{it}; \\gamma)$ derived above.\n",
    "\n",
    "2. GMM estimation based on the conditional mean restriction \n",
    "\t$$E[d_{it} - P(x_{it}, z_{it}; \\gamma) | x_{it}, z_{it}] = 0.$$ \n",
    "\n",
    "In this case, given that we have a fully specified choice model, CMLE will be more efficient, but we also consider GMM for illustrative purposes.\n",
    "\n",
    "To gain a sense for how the estimators compare, we will simulate several Monte Carlo datasets, then explore the performance of each estimator in these simulations. \n",
    "I will provide code for this exercise in Julia, although (for those using other programs) it may be worthwhile to replicate this exercise in other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Drawing data from the model.\n",
    "\n",
    "We first write a few simple functions to generate simulated data from the model above. These use functionality provided by several packages in the Julia language, which we load next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "using Distributions, LinearAlgebra, DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next specify the parameters of the data generating process. We specify the parameters governing service choice as $\\gamma = (5.0, 1.0, 0.2)$. We draw initial mileage $x_{i0}$ from an exponential distribution with mean $60$. We assume the monthly mileage for each individual evolves as $x_{i,t+1} = x_{it} + \\Delta x_{it}$, with $\\Delta x_{it}$ drawn from an exponential distribution with mean $1$. We specify these distribution objects below (note that f_x0 and f_dx defined below are *distribution objects*, which we subsequently feed into a random number generator to draw variables from the relevant distributions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(gamma0) = (3,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Exponential{Float64}(Î¸=1.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choice probability parameters (object of interest)\n",
    "gamma0 = [5.; 1; .2]\n",
    "@show size(gamma0)\n",
    "\n",
    "# Initial mileage distribution (used to simulate data, but not in estimation)\n",
    "f_x0 = Exponential(60.)\n",
    "\n",
    "# Distribution of monthly mileage increment (also used to simulate data, but not in estimate)\n",
    "f_dx = Exponential(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define a Julia function computing the predicted probability that individual $i$ in period $t$ chooses to take their car to a dealership for service. This function will be the core component of our algorithm. To facilitate use in both simulation and estimation, we write the function to take arguments $\\gamma$, the parameters governing choice, and $w = (-1, z, z*\\mathbb{I}[x \\geq 100])$, the vector of observed covariates which affect the choice probability. \n",
    "\n",
    "One technical note in computing predicted choice probabilities: employing $\\gamma$ and $w$ just defined, we may rewrite the predicted choice probability function defined above as\n",
    "\n",
    "\\begin{align}\n",
    "    P(w; \\gamma) &= \\frac{\\exp(w'\\gamma)}{\\exp(0) + \\exp(w'\\gamma)} \\\\\n",
    "    &= \\frac{\\exp(w'\\gamma - \\bar{v})}{\\exp(-\\bar{v}) + \\exp(w'\\gamma - \\bar{v})},\n",
    "\\end{align}\n",
    "\n",
    "where $\\bar{v} = \\max \\{0, w'\\gamma\\}$ is the maximum of the average net utility associated with $d=0$ (i.e., 0.) and the average net utility associated with $d=1$ (i.e., $w'\\gamma$). The latter transformation is useful to ensure numerical stability, as for some values of $w$ and $\\gamma$, the product $w'\\gamma$ could become very large, so that $\\exp(w'\\gamma)$ becomes machine infinity. Normalizing by the maximum of mean utilities prevents such numerical overflow, and ensures that the predicted choice probability $P(w, \\gamma)$ is always numerically stable. This is good programming practice when working with logit models, and is illustrated in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_service_prob (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute predicted probability of taking car for service\n",
    "function predicted_service_prob(gamma, w)\n",
    "    wg = dot(gamma, w)\n",
    "    vbar = max(wg, 0.)\n",
    "    expnormv0 = exp(-vbar)\n",
    "    expnormv1 = exp(wg - vbar)\n",
    "    prob1 = expnormv1 / (expnormv0 + expnormv1)\n",
    "    return(prob1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write a function to draw data from the model above. This function takes arguments nI (the number of individuals) and nT (the number of periods per individual). It returns as output a data frame with columns :I, an individual identifier, :T, a time identifier, :P, the true predicted individual choice probability (not observed in actuality), :D, the observed individual decision, :X, the beginning-of-period cumulative mileage, :Z, the beginning-of-period miles since last service, and :W1-:W3, containing the variables $w_{it}$ for each observation. (The notation :X denotes a *symbol* in Julia -- that is, a unique precompiled identifer, in this case a column name.)\n",
    "\n",
    "Note that we loop over both i and t in simulating data below, computing predicted choice probabilities separately for each individual. This would be a very inefficient construction in Matlab or Python, which tend to slow down dramatically in loops. But the just-in-time compiliation built in to Julia allows loops to execute with little overhead, greatly simplifying efficient coding of inherently recursive operations such as simulation of sequential choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "draw_data (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw data from model (assuming we start following each individual one period after last service)\n",
    "function draw_data(nI, nT)\n",
    "    \n",
    "    # Pre-initialize output matrices\n",
    "    I = zeros(nI*nT)\n",
    "    T = zeros(nI*nT)\n",
    "    P = zeros(nI*nT)\n",
    "    D = zeros(nI*nT)\n",
    "    X = zeros(nI*nT)\n",
    "    Z = zeros(nI*nT)\n",
    "    W = zeros(nI*nT, 3)\n",
    "    w_it = zeros(3)\n",
    "    \n",
    "    # Loop through i by t and simulate data\n",
    "    it = 1\n",
    "    for ii=1:nI\n",
    "        \n",
    "        # initialize x_i, z_i, w_i for this i, x_i0 drawn from f_x0, z_it drawn one period after last service\n",
    "        x_it = rand(f_x0)\n",
    "        z_it = rand(f_dx)\n",
    "        w_it = [-1. z_it z_it*(x_it > 100.)]\n",
    "        \n",
    "        # Loop through periods for this i and update x_i, z_i\n",
    "        for tt=1:nT\n",
    "            \n",
    "            # Fill identifier variables for observation it\n",
    "            I[it] = ii\n",
    "            T[it] = tt\n",
    "            \n",
    "            # Fill beginning-of-period state variables for obs it\n",
    "            X[it] = x_it\n",
    "            Z[it] = z_it\n",
    "            W[it, :] = w_it\n",
    "            \n",
    "            # Compute true predicted probability of service P_it (unobserved)\n",
    "            P[it] = predicted_service_prob(gamma0, w_it)\n",
    "            \n",
    "            # Determine whether service is actually chosen: equivalent to U[0, 1] < P_it\n",
    "            D[it] = rand() < P[it]\n",
    "            \n",
    "            # Increment next period mileage x: x' = x + dx, dx drawn from f_dx\n",
    "            dx = rand(f_dx)\n",
    "            x_it += dx\n",
    "            \n",
    "            # Increment next period z: z' = 0 + dx if service, z' = z + dx otherwise\n",
    "            z_it = (D[it] > 0) ? dx : z_it + dx\n",
    "            \n",
    "            # Update w_it for start of next period and increment counter it\n",
    "            w_it[2] = z_it\n",
    "            w_it[3] = z_it * (x_it >= 100.)\n",
    "            it += 1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Create data frame composed of variables above\n",
    "    #   Note: the exclamation point is Julia syntax for modifying an aspect of an object in place\n",
    "    #   In this case, we first the raw data as a matrix without labels\n",
    "    #   We then update names of each column in the second line\n",
    "    data = DataFrame([I T P D X Z W])\n",
    "    rename!(data, [:I; :T; :P; :D; :X; :Z; Symbol.(:W, 1:3)])\n",
    "    return(data)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>I</th><th>T</th><th>P</th><th>D</th><th>X</th><th>Z</th><th>W1</th><th>W2</th><th>W3</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>10,000 rows Ã 9 columns</p><tr><th>1</th><td>1.0</td><td>1.0</td><td>0.00855888</td><td>0.0</td><td>48.6455</td><td>0.24781</td><td>-1.0</td><td>0.24781</td><td>0.0</td></tr><tr><th>2</th><td>1.0</td><td>2.0</td><td>0.0132955</td><td>0.0</td><td>49.0907</td><td>0.693056</td><td>-1.0</td><td>0.693056</td><td>0.0</td></tr><tr><th>3</th><td>1.0</td><td>3.0</td><td>0.0410922</td><td>0.0</td><td>50.2477</td><td>1.85002</td><td>-1.0</td><td>1.85002</td><td>0.0</td></tr><tr><th>4</th><td>1.0</td><td>4.0</td><td>0.197331</td><td>0.0</td><td>51.9946</td><td>3.59694</td><td>-1.0</td><td>3.59694</td><td>0.0</td></tr><tr><th>5</th><td>1.0</td><td>5.0</td><td>0.353564</td><td>1.0</td><td>52.7943</td><td>4.39659</td><td>-1.0</td><td>4.39659</td><td>0.0</td></tr><tr><th>6</th><td>1.0</td><td>6.0</td><td>0.0570623</td><td>0.0</td><td>54.9894</td><td>2.19514</td><td>-1.0</td><td>2.19514</td><td>0.0</td></tr><tr><th>7</th><td>1.0</td><td>7.0</td><td>0.150729</td><td>0.0</td><td>56.0654</td><td>3.2711</td><td>-1.0</td><td>3.2711</td><td>0.0</td></tr><tr><th>8</th><td>1.0</td><td>8.0</td><td>0.157903</td><td>0.0</td><td>56.1204</td><td>3.32609</td><td>-1.0</td><td>3.32609</td><td>0.0</td></tr><tr><th>9</th><td>1.0</td><td>9.0</td><td>0.191536</td><td>0.0</td><td>56.3542</td><td>3.55994</td><td>-1.0</td><td>3.55994</td><td>0.0</td></tr><tr><th>10</th><td>1.0</td><td>10.0</td><td>0.401572</td><td>0.0</td><td>57.3954</td><td>4.60108</td><td>-1.0</td><td>4.60108</td><td>0.0</td></tr><tr><th>11</th><td>2.0</td><td>1.0</td><td>0.0205232</td><td>0.0</td><td>66.4522</td><td>1.13454</td><td>-1.0</td><td>1.13454</td><td>0.0</td></tr><tr><th>12</th><td>2.0</td><td>2.0</td><td>0.066256</td><td>0.0</td><td>67.672</td><td>2.35432</td><td>-1.0</td><td>2.35432</td><td>0.0</td></tr><tr><th>13</th><td>2.0</td><td>3.0</td><td>0.0895546</td><td>1.0</td><td>67.9986</td><td>2.68091</td><td>-1.0</td><td>2.68091</td><td>0.0</td></tr><tr><th>14</th><td>2.0</td><td>4.0</td><td>0.0690742</td><td>0.0</td><td>70.3976</td><td>2.399</td><td>-1.0</td><td>2.399</td><td>0.0</td></tr><tr><th>15</th><td>2.0</td><td>5.0</td><td>0.17921</td><td>0.0</td><td>71.4769</td><td>3.47829</td><td>-1.0</td><td>3.47829</td><td>0.0</td></tr><tr><th>16</th><td>2.0</td><td>6.0</td><td>0.277203</td><td>0.0</td><td>72.0402</td><td>4.04162</td><td>-1.0</td><td>4.04162</td><td>0.0</td></tr><tr><th>17</th><td>2.0</td><td>7.0</td><td>0.923033</td><td>1.0</td><td>75.4829</td><td>7.48429</td><td>-1.0</td><td>7.48429</td><td>0.0</td></tr><tr><th>18</th><td>2.0</td><td>8.0</td><td>0.0152359</td><td>0.0</td><td>76.3142</td><td>0.831253</td><td>-1.0</td><td>0.831253</td><td>0.0</td></tr><tr><th>19</th><td>2.0</td><td>9.0</td><td>0.015248</td><td>0.0</td><td>76.315</td><td>0.83206</td><td>-1.0</td><td>0.83206</td><td>0.0</td></tr><tr><th>20</th><td>2.0</td><td>10.0</td><td>0.0236836</td><td>0.0</td><td>76.7639</td><td>1.281</td><td>-1.0</td><td>1.281</td><td>0.0</td></tr><tr><th>21</th><td>3.0</td><td>1.0</td><td>0.013723</td><td>0.0</td><td>61.3417</td><td>0.725139</td><td>-1.0</td><td>0.725139</td><td>0.0</td></tr><tr><th>22</th><td>3.0</td><td>2.0</td><td>0.0697458</td><td>0.0</td><td>63.026</td><td>2.4094</td><td>-1.0</td><td>2.4094</td><td>0.0</td></tr><tr><th>23</th><td>3.0</td><td>3.0</td><td>0.127841</td><td>1.0</td><td>63.6964</td><td>3.07982</td><td>-1.0</td><td>3.07982</td><td>0.0</td></tr><tr><th>24</th><td>3.0</td><td>4.0</td><td>0.00899042</td><td>0.0</td><td>63.9939</td><td>0.297435</td><td>-1.0</td><td>0.297435</td><td>0.0</td></tr><tr><th>25</th><td>3.0</td><td>5.0</td><td>0.0190063</td><td>0.0</td><td>64.7526</td><td>1.0562</td><td>-1.0</td><td>1.0562</td><td>0.0</td></tr><tr><th>26</th><td>3.0</td><td>6.0</td><td>0.0813811</td><td>0.0</td><td>66.2727</td><td>2.57627</td><td>-1.0</td><td>2.57627</td><td>0.0</td></tr><tr><th>27</th><td>3.0</td><td>7.0</td><td>0.638151</td><td>1.0</td><td>69.2638</td><td>5.56735</td><td>-1.0</td><td>5.56735</td><td>0.0</td></tr><tr><th>28</th><td>3.0</td><td>8.0</td><td>0.00873564</td><td>0.0</td><td>69.5322</td><td>0.26843</td><td>-1.0</td><td>0.26843</td><td>0.0</td></tr><tr><th>29</th><td>3.0</td><td>9.0</td><td>0.0830221</td><td>1.0</td><td>71.8618</td><td>2.59802</td><td>-1.0</td><td>2.59802</td><td>0.0</td></tr><tr><th>30</th><td>3.0</td><td>10.0</td><td>0.0163269</td><td>0.0</td><td>72.7633</td><td>0.901522</td><td>-1.0</td><td>0.901522</td><td>0.0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& I & T & P & D & X & Z & W1 & W2 & W3\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1.0 & 1.0 & 0.00855888 & 0.0 & 48.6455 & 0.24781 & -1.0 & 0.24781 & 0.0 \\\\\n",
       "\t2 & 1.0 & 2.0 & 0.0132955 & 0.0 & 49.0907 & 0.693056 & -1.0 & 0.693056 & 0.0 \\\\\n",
       "\t3 & 1.0 & 3.0 & 0.0410922 & 0.0 & 50.2477 & 1.85002 & -1.0 & 1.85002 & 0.0 \\\\\n",
       "\t4 & 1.0 & 4.0 & 0.197331 & 0.0 & 51.9946 & 3.59694 & -1.0 & 3.59694 & 0.0 \\\\\n",
       "\t5 & 1.0 & 5.0 & 0.353564 & 1.0 & 52.7943 & 4.39659 & -1.0 & 4.39659 & 0.0 \\\\\n",
       "\t6 & 1.0 & 6.0 & 0.0570623 & 0.0 & 54.9894 & 2.19514 & -1.0 & 2.19514 & 0.0 \\\\\n",
       "\t7 & 1.0 & 7.0 & 0.150729 & 0.0 & 56.0654 & 3.2711 & -1.0 & 3.2711 & 0.0 \\\\\n",
       "\t8 & 1.0 & 8.0 & 0.157903 & 0.0 & 56.1204 & 3.32609 & -1.0 & 3.32609 & 0.0 \\\\\n",
       "\t9 & 1.0 & 9.0 & 0.191536 & 0.0 & 56.3542 & 3.55994 & -1.0 & 3.55994 & 0.0 \\\\\n",
       "\t10 & 1.0 & 10.0 & 0.401572 & 0.0 & 57.3954 & 4.60108 & -1.0 & 4.60108 & 0.0 \\\\\n",
       "\t11 & 2.0 & 1.0 & 0.0205232 & 0.0 & 66.4522 & 1.13454 & -1.0 & 1.13454 & 0.0 \\\\\n",
       "\t12 & 2.0 & 2.0 & 0.066256 & 0.0 & 67.672 & 2.35432 & -1.0 & 2.35432 & 0.0 \\\\\n",
       "\t13 & 2.0 & 3.0 & 0.0895546 & 1.0 & 67.9986 & 2.68091 & -1.0 & 2.68091 & 0.0 \\\\\n",
       "\t14 & 2.0 & 4.0 & 0.0690742 & 0.0 & 70.3976 & 2.399 & -1.0 & 2.399 & 0.0 \\\\\n",
       "\t15 & 2.0 & 5.0 & 0.17921 & 0.0 & 71.4769 & 3.47829 & -1.0 & 3.47829 & 0.0 \\\\\n",
       "\t16 & 2.0 & 6.0 & 0.277203 & 0.0 & 72.0402 & 4.04162 & -1.0 & 4.04162 & 0.0 \\\\\n",
       "\t17 & 2.0 & 7.0 & 0.923033 & 1.0 & 75.4829 & 7.48429 & -1.0 & 7.48429 & 0.0 \\\\\n",
       "\t18 & 2.0 & 8.0 & 0.0152359 & 0.0 & 76.3142 & 0.831253 & -1.0 & 0.831253 & 0.0 \\\\\n",
       "\t19 & 2.0 & 9.0 & 0.015248 & 0.0 & 76.315 & 0.83206 & -1.0 & 0.83206 & 0.0 \\\\\n",
       "\t20 & 2.0 & 10.0 & 0.0236836 & 0.0 & 76.7639 & 1.281 & -1.0 & 1.281 & 0.0 \\\\\n",
       "\t21 & 3.0 & 1.0 & 0.013723 & 0.0 & 61.3417 & 0.725139 & -1.0 & 0.725139 & 0.0 \\\\\n",
       "\t22 & 3.0 & 2.0 & 0.0697458 & 0.0 & 63.026 & 2.4094 & -1.0 & 2.4094 & 0.0 \\\\\n",
       "\t23 & 3.0 & 3.0 & 0.127841 & 1.0 & 63.6964 & 3.07982 & -1.0 & 3.07982 & 0.0 \\\\\n",
       "\t24 & 3.0 & 4.0 & 0.00899042 & 0.0 & 63.9939 & 0.297435 & -1.0 & 0.297435 & 0.0 \\\\\n",
       "\t25 & 3.0 & 5.0 & 0.0190063 & 0.0 & 64.7526 & 1.0562 & -1.0 & 1.0562 & 0.0 \\\\\n",
       "\t26 & 3.0 & 6.0 & 0.0813811 & 0.0 & 66.2727 & 2.57627 & -1.0 & 2.57627 & 0.0 \\\\\n",
       "\t27 & 3.0 & 7.0 & 0.638151 & 1.0 & 69.2638 & 5.56735 & -1.0 & 5.56735 & 0.0 \\\\\n",
       "\t28 & 3.0 & 8.0 & 0.00873564 & 0.0 & 69.5322 & 0.26843 & -1.0 & 0.26843 & 0.0 \\\\\n",
       "\t29 & 3.0 & 9.0 & 0.0830221 & 1.0 & 71.8618 & 2.59802 & -1.0 & 2.59802 & 0.0 \\\\\n",
       "\t30 & 3.0 & 10.0 & 0.0163269 & 0.0 & 72.7633 & 0.901522 & -1.0 & 0.901522 & 0.0 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "10000Ã9 DataFrame. Omitted printing of 3 columns\n",
       "â Row   â I       â T       â P          â D       â X       â Z        â\n",
       "â       â \u001b[90mFloat64\u001b[39m â \u001b[90mFloat64\u001b[39m â \u001b[90mFloat64\u001b[39m    â \u001b[90mFloat64\u001b[39m â \u001b[90mFloat64\u001b[39m â \u001b[90mFloat64\u001b[39m  â\n",
       "âââââââââ¼ââââââââââ¼ââââââââââ¼âââââââââââââ¼ââââââââââ¼ââââââââââ¼âââââââââââ¤\n",
       "â 1     â 1.0     â 1.0     â 0.00855888 â 0.0     â 48.6455 â 0.24781  â\n",
       "â 2     â 1.0     â 2.0     â 0.0132955  â 0.0     â 49.0907 â 0.693056 â\n",
       "â 3     â 1.0     â 3.0     â 0.0410922  â 0.0     â 50.2477 â 1.85002  â\n",
       "â 4     â 1.0     â 4.0     â 0.197331   â 0.0     â 51.9946 â 3.59694  â\n",
       "â 5     â 1.0     â 5.0     â 0.353564   â 1.0     â 52.7943 â 4.39659  â\n",
       "â 6     â 1.0     â 6.0     â 0.0570623  â 0.0     â 54.9894 â 2.19514  â\n",
       "â 7     â 1.0     â 7.0     â 0.150729   â 0.0     â 56.0654 â 3.2711   â\n",
       "â 8     â 1.0     â 8.0     â 0.157903   â 0.0     â 56.1204 â 3.32609  â\n",
       "â 9     â 1.0     â 9.0     â 0.191536   â 0.0     â 56.3542 â 3.55994  â\n",
       "â 10    â 1.0     â 10.0    â 0.401572   â 0.0     â 57.3954 â 4.60108  â\n",
       "â®\n",
       "â 9990  â 999.0   â 10.0    â 0.98911    â 1.0     â 46.0033 â 9.50892  â\n",
       "â 9991  â 1000.0  â 1.0     â 0.0153728  â 0.0     â 48.0912 â 0.840339 â\n",
       "â 9992  â 1000.0  â 2.0     â 0.0299091  â 0.0     â 48.7716 â 1.52077  â\n",
       "â 9993  â 1000.0  â 3.0     â 0.0691569  â 0.0     â 49.6511 â 2.40029  â\n",
       "â 9994  â 1000.0  â 4.0     â 0.104305   â 0.0     â 50.1006 â 2.84972  â\n",
       "â 9995  â 1000.0  â 5.0     â 0.146673   â 0.0     â 50.4899 â 3.23906  â\n",
       "â 9996  â 1000.0  â 6.0     â 0.212688   â 0.0     â 50.942  â 3.6912   â\n",
       "â 9997  â 1000.0  â 7.0     â 0.301515   â 0.0     â 51.4107 â 4.15991  â\n",
       "â 9998  â 1000.0  â 8.0     â 0.761036   â 0.0     â 53.4092 â 6.15837  â\n",
       "â 9999  â 1000.0  â 9.0     â 0.89938    â 1.0     â 54.4412 â 7.19036  â\n",
       "â 10000 â 1000.0  â 10.0    â 0.0415748  â 0.0     â 56.3034 â 1.8622   â"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw a representative dataset\n",
    "data = draw_data(1000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: CMLE Estimation\n",
    "\n",
    "We first estimate the model by CMLE. Under the assumptions described above, the choice probability function $P(w_{it}; \\gamma_0)$ completely specifies the marginal density of the choice $d_{it}$ given contemporateous covariates $w_{it}$. Furthermore, by hypothesis, individual $i$'s choice in period $t$ does not depend on lagged realizations of $d_{it}$ and $w_{it}$ for individual $i$, although current $w_{it}$ will generally not be independent of lagged $w_{i,t-1}$ for a given individual. In other words, in the language of panel data analysis, our model is *dynamically complete*, which allows us to apply standard MLE asymptotics.\n",
    "\n",
    "(In this context, dynamic completeness essentially requires that, conditional on current covariates $w_{it}$, current choices $d_{it}$ are independent of past choices $d_{i,t-\\tau}$ and past covariates $w_{i,t-\\tau}$. Since we have assumed independence of utility shocks $\\epsilon_{i0t}, \\epsilon_{i1t}$ across periods, this is true in our context. If these errors were not independent, but the marginal choice probability function $P(w_{it}; \\gamma_0)$ were otherwise correctly specified, we could consider *partial MLE analysis*, which involves maximizing the same marginal objective function, but requires a general quasi-MLE approach asymptotic inference since the conditional information matrix no longer holds. See Wooldridge 13.8 for a detailed discussion.) \n",
    "\n",
    "By definition, the CMLE estimator maximizes the sum of observation-level log-likelihoods, in this case taken across both individuals $i$ and periods $t$. In this case, the observation-level log likelihood is the log of the probability of observing choice $d_{it}$ given the observed covariates $w_{it}$. Bearing in mind that $d_{it}$ is either zero or one, we may write this individual log likelihood concisely as\n",
    "$$\n",
    "    \\ell_i(\\gamma) = d_{it} \\log(P(w_{it}; \\gamma)) + (1-d_{it}) \\log(1 - P(w_{it}; \\gamma)).\n",
    "$$\n",
    "Summing across individuals and periods gives the sample log likelihood: \n",
    "$$\n",
    "    \\mathcal{L}_N(\\gamma) = \\sum_{i=1}^N \\sum_{t=1}^T \\ell_i(\\gamma).\n",
    "$$\n",
    "By definition, the CMLE estimator $\\hat{\\gamma}$ maximizes the sample log likelihood $\\mathcal{L}_N(\\gamma)$:\n",
    "$$\n",
    "    \\hat{\\gamma} = \\arg \\max_{\\gamma} \\mathcal{L}_N(\\gamma).\n",
    "$$\n",
    "To find this maximum, we will need functions for calculating the value, gradient (sum of scores), and Hessian of $\\mathcal{L}_N(\\gamma)$. We define these functions next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the sample log-likelihood\n",
    "\n",
    "We first define functions for calculating the individual and sample log-likelihoods. These are straightforward applications of the formulas above. In computing the sample log likelihood, however, we apply one special wrinkle in initializing the output vector which allows Julia to determine the type of the function output dynamically. This allows us to apply automatic differentiation methods to compute the gradient as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_log_likelihood (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute observation (it) level log likelihood: log(P_it) if d_it > 0, log(1-P_it) else\n",
    "function period_log_likelihood(gamma, d_it, w_it)\n",
    "    ccp = predicted_service_prob(gamma, w_it)\n",
    "    if d_it > 0\n",
    "        return log(ccp)\n",
    "    else\n",
    "        return log(1 - ccp)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Compute sample log likelihood: summing over periods\n",
    "function sample_log_likelihood(gamma, data)\n",
    "    \n",
    "    # Retrieve relevant columns of data frame in matrix form\n",
    "    D = data[!, :D]\n",
    "    W = convert(Matrix, data[:, Symbol.(:W, 1:3)])\n",
    "    \n",
    "    # Initialize log likelihood to first value\n",
    "    #  Note: we compute this separately to allow output type to be determined by Julia\n",
    "    #  As opposed, for example, to initializing sumll=0., which forces sumll to be a float\n",
    "    #  This doesn't matter for computing the numeric value of the log-likelihood\n",
    "    #  But it is required if we want to use the automatic differentiation methods employed below\n",
    "    sumll = period_log_likelihood(gamma, D[1], W[1,:])\n",
    "    \n",
    "    # Loop over remaining observations and compute overall log likelihood\n",
    "    for it=2:length(D)\n",
    "        sumll += period_log_likelihood(gamma, D[it], W[it, :])\n",
    "    end\n",
    "    \n",
    "    # Return sum log likelihood\n",
    "    return(sumll)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next write a function to compute the value, gradient and Hessian of the log-likelihood function simultaneously. We use the ForwardDiff package in Julia to compute gradients and Hessians automatically -- a powerful tool based on the fact that all machine calculations ultimately boil down to addition, subtraction, multiplication and division (so the chain rule can be applied at the machine operation level). My implementation requires two additional packages, ForwardDiff and DiffResults, whose use is illustrated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_score_hessian (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ForwardDiff, DiffResults\n",
    "\n",
    "# Compute sample score and Hessian\n",
    "#   Here we use the ForwardDiff package, which allows efficient automatic computation of gradients and hessians\n",
    "function sample_score_hessian(gamma, data)\n",
    "    \n",
    "    # We first initialize a HessianResult structure that allows us to compute objective, score, and Hessian in one shot\n",
    "    #  Here the argument gamma describes the vector with which we aim to take derivatives\n",
    "    hessres = DiffResults.HessianResult(gamma)\n",
    "    \n",
    "    # We now apply the ForwardDiff.hessian! method to compute the gradient and Hessian of the log-likelihood\n",
    "    #  We first specify the log-likelihood as an anonymous function of gamma only\n",
    "    #  We then call ForwardDiff.hessian! to fill the results of hessres\n",
    "    #  Note the !, which specifies that this function will modify one of its arguments\n",
    "    #  In this case, it will modify the hessres structure, which will ultimately contain the outputs\n",
    "    func = g -> sample_log_likelihood(g, data)\n",
    "    ForwardDiff.hessian!(hessres, func, gamma)\n",
    "    \n",
    "    # Finally, we retrieve the objective, score and gradient from the HessRes structure\n",
    "    sumll = DiffResults.value(hessres)\n",
    "    sumscore = DiffResults.gradient(hessres)\n",
    "    sumhessian = DiffResults.hessian(hessres)\n",
    "    return(sumll, sumscore, sumhessian)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.758366 seconds (5.62 M allocations: 284.918 MiB, 4.90% gc time)\n",
      "  0.097712 seconds (412.11 k allocations: 20.283 MiB, 6.11% gc time)\n",
      "  0.010597 seconds (98.06 k allocations: 5.468 MiB)\n",
      "  0.009583 seconds (98.04 k allocations: 3.025 MiB)\n",
      "  0.011550 seconds (98.06 k allocations: 5.468 MiB)\n",
      "  0.009384 seconds (98.04 k allocations: 3.025 MiB)\n",
      "  0.018418 seconds (98.06 k allocations: 5.468 MiB, 36.66% gc time)\n",
      "  0.010131 seconds (98.04 k allocations: 3.025 MiB)\n",
      "  0.009655 seconds (98.06 k allocations: 5.468 MiB)\n",
      "  0.008897 seconds (98.04 k allocations: 3.025 MiB)\n",
      "  0.009584 seconds (98.06 k allocations: 5.468 MiB)\n",
      "sumll = -2854.8435271716125\n",
      "sumscore = [-0.2401, 79.7537, 7.99361]\n",
      "sumhess = [-891.893 3380.51 688.996; 3380.51 -14558.0 -2637.34; 688.996 -2637.34 -2637.34]\n"
     ]
    }
   ],
   "source": [
    "# Test the log-likelihood, score, and Hessian functions\n",
    "#   Note the large difference in time to run between the first call of sample_score_hessian and subsequent calls\n",
    "#   This is a function of Julia's just-in-time compiler, which must run once the first time the function is called\n",
    "#   Note further that (at least in this example) there's hardly any difference between the time required to \n",
    "#   evaluate the log-likelihood only, and the time required to additionally compute the score and Hessian\n",
    "@time sumll, sumscore, sumhess = sample_score_hessian(gamma0, data)\n",
    "for ii=1:5\n",
    "    gamma1 = gamma0 + .1*rand(length(gamma0))\n",
    "    @time sample_log_likelihood(gamma1, data)\n",
    "    @time sumll1, sumscore1, sumhess1 = sample_score_hessian(gamma1, data);\n",
    "end\n",
    "@show sumll;\n",
    "@show sumscore;\n",
    "@show sumhess;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also write a function to compute a matrix estimating the expected outer product of individual scores. Recall that $x_{it}$'s are only independent across $i$, not across $t$. Hence, formally speaking, we want an estimator of\n",
    "$$\n",
    "    B_0 = E[s_{i}(\\gamma)s_{i}(\\gamma)'], \n",
    "$$\n",
    "where $s_i(\\gamma)$ denotes the sum of scores for individual $i$:\n",
    "$$\n",
    "    s_i(\\gamma) = \\sum_{t=1}^{T} s_{it}(\\gamma).\n",
    "$$\n",
    "In other words, formally speaking, we want the average of the outer products of individual-level scores $s_{i}(\\gamma)$ across individuals $i$, rather than the average of the outer products of the individual-period scores $s_{it}(\\gamma)$ across both $i$ and $t$. However, since the model satisfies dynamic completeness, one can show (see Wooldridge 13.8.3) that all interactions of cross-time scores have mean zero, from which it follows that\n",
    "$$\n",
    "    E[s_{i}(\\gamma_0)s_{i}(\\gamma_0)'] = E\\left[\\sum_{t=1}^T s_{it}(\\gamma_0) s_{it}(\\gamma_0)'\\right]. \n",
    "$$\n",
    "In other words, we may estimate $B_0$ by \n",
    "$$\n",
    "    \\hat{B} = \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T s_{it}(\\gamma_0) s_{it}(\\gamma_0)',\n",
    "$$\n",
    "i.e., by taking the sum of the outer product of observation-level scores $s_{it}(\\gamma_0)$ across observations $i$ and $t$, then normalizing by the number of individuals $N$. \n",
    "\n",
    "In practice, we simply compute a non-normalized sum of outer products; we normalize later. In other words, we effectively compute $N\\cdot \\hat{B}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.436203 seconds (1.59 M allocations: 83.354 MiB, 6.41% gc time)\n"
     ]
    }
   ],
   "source": [
    "function get_sum_outer_prod_scores(gamma, data)\n",
    "    \n",
    "    # Retrieve data in matrix form\n",
    "    D = data[!, :D]\n",
    "    W = convert(Matrix, data[:, Symbol.(:W, 1:3)])\n",
    "    \n",
    "    # Compute period-level scores using ForwardDiff\n",
    "    sum_outer_prod_score = zeros(length(gamma), length(gamma))\n",
    "    for it=1:length(D)\n",
    "        func = g -> period_log_likelihood(g, D[it], W[it,:])\n",
    "        score_it = ForwardDiff.gradient(func, gamma)\n",
    "        sum_outer_prod_score += score_it*score_it'\n",
    "    end\n",
    "    return(sum_outer_prod_score)\n",
    "end\n",
    "\n",
    "@time sumouterprodscore = get_sum_outer_prod_scores(gamma0, data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.019192 seconds (109.07 k allocations: 9.297 MiB, 40.63% gc time)\n",
      "  0.011296 seconds (109.07 k allocations: 9.297 MiB)\n",
      "  0.015417 seconds (109.07 k allocations: 9.297 MiB, 33.73% gc time)\n",
      "  0.010088 seconds (109.07 k allocations: 9.297 MiB)\n",
      "  0.009216 seconds (109.07 k allocations: 9.297 MiB)\n",
      "  0.014153 seconds (109.07 k allocations: 9.297 MiB, 30.00% gc time)\n",
      "  0.011077 seconds (109.07 k allocations: 9.297 MiB)\n",
      "  0.013846 seconds (109.07 k allocations: 9.297 MiB, 27.83% gc time)\n",
      "  0.009848 seconds (109.07 k allocations: 9.297 MiB)\n",
      "  0.013575 seconds (109.07 k allocations: 9.297 MiB, 28.87% gc time)\n"
     ]
    }
   ],
   "source": [
    "# Out of curiosity, let's compare the execution time of outer prod scores to the full Hessian\n",
    "# In this example, apart from the initial compile time, there doesn't appear to be much difference!\n",
    "for ii=1:10\n",
    "    gamma1 = gamma0 + .1*rand(length(gamma0))\n",
    "    @time sumouterprodscore = get_sum_outer_prod_scores(gamma1, data)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just for fun, let's verify the information matrix equality. Using the functions above, we estimate both $\\hat{A}$ and $\\hat{B}$, and compute the elementwise percentage difference between $\\hat{A}$ and $\\hat{B}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3Ã3 Array{Float64,2}:\n",
       " -0.00797296  -0.0091831   -0.0469244\n",
       " -0.0091831   -0.00787872  -0.0509885\n",
       " -0.0469244   -0.0509885   -0.0509885"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check information matrix equality elementwise \n",
    "N = size(data, 1)\n",
    "sumll, sumscore, sumhess = sample_score_hessian(gamma0, data)\n",
    "Ahat = -sumhess / N\n",
    "Bhat = get_sum_outer_prod_scores(gamma0, data) / N\n",
    "(Ahat - Bhat) ./ Ahat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the maximization problem\n",
    "\n",
    "We now have functions to compute the log-likelihood, score, and Hessian of the MLE problem. Let $\\mathcal{S}_N(\\gamma)$ and $\\mathcal{H}_N(\\gamma)$, respectively, denote the sum of scores and sum of Hessians in the sample at parameter value $\\gamma$. We aim to find the parameter $\\hat{\\gamma}$ solving $\\max_{\\gamma} \\mathcal{L}_N(\\gamma)$. \n",
    "\n",
    "Toward this end, we will use Newton-Raphson type iteration to solve the maximization problem. For illustrative purposes, we will code the maximization algorithms ourselves. Julia, like Matlab, also has pre-coded maximization functions; we'll compare our solution to these below. \n",
    "\n",
    "Newton-Raphson type algorithms are based on the following iteration strategy. We know the optimum $\\hat{\\gamma}$ satisfies \n",
    "$$\n",
    "\\mathcal{S}_N(\\hat{\\gamma}) = 0. \n",
    "$$\n",
    "Let $\\gamma^k$ be some guess for the maximizer $\\hat{\\gamma}$. We seek to find a next step $\\gamma^{k+1}$ such that the score function $\\mathcal{S}_N(\\gamma^{k+1})$ is as close as possible to zero. Toward this end, consider the Taylor expansion of $\\mathcal{S}_N(\\gamma)$ around our current guess $\\gamma^k$: \n",
    "$$\n",
    "\\mathcal{S}_N(\\gamma^{k+1}) \\approx \\mathcal{S}_N(\\gamma^{k}) + \\mathcal{H}_N(\\gamma^{k})(\\gamma^{k+1} - \n",
    "\\gamma^k).\n",
    "$$\n",
    "Bearing this approximation in mind, we therefore choose the next step $\\gamma^{k+1}$ to make the right-hand side of the Taylor approximation zero, which (we hope) implies $\\mathcal{S}_N(\\gamma^{k+1})$ close to zero. Solving for $\\gamma^{k+1}$ from the resulting expression, we ultimately obtain:\n",
    "$$\n",
    "\\gamma^{k+1} = \\gamma^k + \\left[-\\mathcal{H}_N(\\gamma^{k})\\right]^{-1} \\mathcal{S}_N(\\gamma^{k}).\n",
    "$$\n",
    "For any initial guess $\\gamma^{0}$ \"close enough\" to $\\hat{\\gamma}$, this iterative scheme will yield a sequence $\\gamma^{k}$ converging to the optimum $\\hat{\\gamma}$ at a *quadratic* rate: that is, the error in step $k+1$ will decrease with the square of the step-$k$ error (which is very fast when the current error is small!). Unfortunately, for \"bad\" initial guesses (i.e., those outside an a priori unknown neighborhood of convergence), the pure NR algorithm is not guaranteed to converge at all -- especially when the objective function is not globally concave. Furthermore, in some cases (though not here), the Hessian can be costly to code and / or evaluate. \n",
    "\n",
    "For these reasons, in practice, numerical optimization procedures typically employ two modifications to the baseline NR algorithm. First, in place of the exact Hessian $\\mathcal{H}_N(\\gamma^{k})$, it is common to employ another matrix $H^k$ --- typically an approximation of the Hessian which is either (a) easier to compute or (b) guaranteed to be negative definite. This yields a baseline step $\\Delta^k$ given by\n",
    "$$\n",
    "\\Delta^k = [-H^k]^{-1} \\mathcal{S}_N(\\gamma^{k}).\n",
    "$$\n",
    "Second, rather than take the baseline step $\\Delta^k$ directly, it is typical to scale the actual step size by a constant $\\alpha^k$, so that the actual iteration is \n",
    "$$\n",
    "    \\gamma^{k+1} = \\gamma^k + \\alpha^k \\Delta^k.\n",
    "$$\n",
    "In practice, the step length $\\alpha^k$ is typically determined by a line search along the direction $\\Delta^k$ for a suitable improvement in the objective function $\\mathcal{L}_N(\\gamma^k + \\alpha \\Delta^k)$. We implement a very simplistic version of this line search (stop when any improvement found) below.\n",
    "\n",
    "Theoretically, such Newton-Raphson type algorithms have the following convergence properties:\n",
    "\n",
    "- So long as the search direction $\\Delta^k$ satisfies $\\mathcal{S}_N(\\gamma^{k})' \\cdot \\Delta^k > 0$, there will exist $\\alpha^k > 0$ such that $\\mathcal{L}_N(\\gamma^k + \\alpha^k \\Delta^k) \\geq \\mathcal{L}_N(\\gamma^k)$. In other words, so long as the search direction $\\Delta^k$ is in the same half-space as the gradient $\\mathcal{S}_N(\\gamma^{k})$, sufficiently small steps along the search direction will improve the objective function. Observe that since $\\mathcal{S}_N(\\gamma^{k})' \\Delta^k = [-H^k]^{-1} \\mathcal{S}_N(\\gamma^{k})$, we can guarantee $\\mathcal{S}_N(\\gamma^{k})' \\Delta^k > 0$ by ensuring that the Hessian approximator $H^k$ is negative definite ate each iteration $k$. \n",
    "\n",
    "- So long as at each iteration $k$ the step length $\\alpha^k$ is chosen such that the step $\\alpha^k \\Delta^k$ leads to an improvement in the objective function, any NR-type algorithm will eventually converge to a local maximum. \n",
    "\n",
    "In practice, there are many ways to choose the Hessian approximator $H^k$. Some leading examples are:\n",
    "\n",
    "1. The exact Hessian, potentially adjusted to ensure negative definiteness if necessary: e.g. $H^k = \\mathcal{H}_N(\\gamma^{k}) - \\lambda^k I$, where $\\lambda^k \\geq 0$ is chosen to ensure negative definiteneness (e.g., $\\lambda^k = 0$ if $\\mathcal{H}_N(\\gamma^{k})$ is already negative definite, otherwise $\\lambda^k$ strictly exceeds the maximum positive eigenvalue of $\\mathcal{H}_N(\\gamma^{k})$). \n",
    "\n",
    "2. For MLE models, the BerndtâHallâHallâHausman (BHHH) step is available, which defines $-H^k$ as the sum of outer products of scores computed above. This substitution is justified by the information matrix equality, which tells us that close to the optimum the negative expected Hessian $-A_0$ should equal the expected score variance $B_0$. For MLE models, the BHHH step has two practical advantages. First, it only requires us to evaluate scores, not Hessians. Second, since the outer product of scores is always positive definite, the BHHH approximation ensures that we can always find an improvement in the objective function along the BHHH step direction.\n",
    "\n",
    "3. The BroydenâFletcherâGoldfarbâShanno (BFGS) algorithm, which constructs an iterative approximation $H^k$ to the actual Hessian $\\mathcal{H}_N(\\gamma^{k})$ using only information on the score $\\mathcal{S}_N(\\gamma^{k})$ at each step, in such a way that $-H^k$ is always positive definite. Computationally, the BFGS algorithm is attractive in settings where the exact Hessian $\\mathcal{H}_N(\\gamma^{k})$ is difficult to compute, since BFGS requires only information on the score (which we need to compute anyway) to construct the approximation $H^k$. Furthermore, as with BHHH, BFGS ensures that we can always find an improvement along the search direction. \n",
    "\n",
    "For this problem, we have already computed both the Hessian and outer product of scores, so both the exact Hessian and BHHH approaches are available. We explore both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "  0.188535 seconds (1.27 M allocations: 70.714 MiB, 9.18% gc time)\n",
      "llhat = -2948.622769803263\n",
      "scorehat = [2.04911e-10, 2.05054e-10, 1.88041e-10]\n"
     ]
    }
   ],
   "source": [
    "# Solve MLE problem using hybrid of pure Hessian and BHHH approaches\n",
    "function solve_mle_problem(gamma0, data, usebhhh=false)\n",
    "    maxit = 1000\n",
    "    ll0, score0, hess0 = sample_score_hessian(gamma0, data)\n",
    "    for ii=1:maxit\n",
    "        \n",
    "        # If useBHHH option is flagged: use BHHH hessian step\n",
    "        if usebhhh\n",
    "            hess0 = -get_sum_outer_prod_scores(gamma0, data)\n",
    "\n",
    "        # Else check whether Hessian is negative definite; if not, fall back on BHHH\n",
    "        elseif !isposdef(-hess0) \n",
    "            hess0 = -get_sum_outer_prod_scores(gamma0, data)\n",
    "            print(\"Hessian is not negative definite! Falling back on BHHH.\\n\")\n",
    "        end\n",
    "        \n",
    "        # Compute initial NR-type step\n",
    "        #   Note: hess \\ score (i.e. solve H step = score)) is both more stable and more efficient \n",
    "        #   than inv(hess)*score.\n",
    "        step = -hess0 \\ score0\n",
    "        \n",
    "        # If initial step length > 1: normalize to max length 1 (prevent crazy steps)\n",
    "        stepnorm = max(1., norm(step))\n",
    "        step ./ stepnorm\n",
    "        \n",
    "        # Cut back step until log-likelhood improvement found (or 10 iterations, whichever happens first)\n",
    "        gamma1 = gamma0 + step\n",
    "        ll1 = sample_log_likelihood(gamma1, data)\n",
    "        for ctr = 1:10\n",
    "            if (ll1 > ll0)\n",
    "                break\n",
    "            end\n",
    "            step ./ 2\n",
    "            gamma1 = gamma0 + step\n",
    "            ll1 = sample_log_likelihood(gamma1, data)\n",
    "        end\n",
    "        \n",
    "        # Update current values \n",
    "        gamma0 = gamma1\n",
    "        ll0, score0, hess0 = sample_score_hessian(gamma0, data)\n",
    "        \n",
    "        # Display status every 10 iterations\n",
    "        if (ii % 10) == 0\n",
    "            @show ii, ll0, score0\n",
    "        end\n",
    "        \n",
    "        # Check for convergence: norm of score less than 1e-6\n",
    "        if norm(score0) < 1e-6\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Return results\n",
    "    return(gamma0, ll0, score0, hess0)\n",
    "end\n",
    "\n",
    "# Find MLE solution from the true parameters\n",
    "@time gammahat, llhat, scorehat, hesshat = solve_mle_problem(gamma0, data)\n",
    "@show llhat;\n",
    "@show scorehat;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "Hessian is not negative definite! Falling back on BHHH.\n",
      "  0.338460 seconds (2.85 M allocations: 165.607 MiB, 9.72% gc time)\n",
      "  0.285087 seconds (2.84 M allocations: 165.583 MiB, 9.31% gc time)\n",
      "llhat = -2948.6227698032194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2948.6227698032194"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify MLE solution from different starting points\n",
    "gamma1 = gamma0 + randn(size(gamma0))\n",
    "\n",
    "# Solve baseline algorithm using exact Hessian where possible\n",
    "@time gammahat, llhat, scorehat, hesshat = solve_mle_problem(gamma1, data, false)\n",
    "\n",
    "# Solve using BHHH approximation at each step\n",
    "@time gammahat, llhat, scorehat, hesshat = solve_mle_problem(gamma1, data, true)\n",
    "\n",
    "@show llhat;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Inference\n",
    "\n",
    "Finally, we compute asymptotic MLE confidence intervals. We consider both the $\\hat{A}$ and $\\hat{B}$ versions, and compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3Ã3 Array{Float64,2}:\n",
       " 0.0094034     0.00211553    0.000271268\n",
       " 0.00211553    0.000558838  -2.18682e-5 \n",
       " 0.000271268  -2.18682e-5    0.000449906"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asymptotic VCV based on Hessian matrix\n",
    "hesshat = sample_score_hessian(gammahat, data)[3]\n",
    "VCVh = -hesshat \\ I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3Ã3 Array{Float64,2}:\n",
       " 0.00961409    0.00216055    0.000316472\n",
       " 0.00216055    0.000568779  -1.21221e-5 \n",
       " 0.000316472  -1.21221e-5    0.000445223"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asymptotic VCV based on score variance matrix\n",
    "vshat = get_sum_outer_prod_scores(gammahat, data)\n",
    "VCVvs = vshat \\ I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Coef</th><th>Est</th><th>se_h</th><th>se_vs</th><th>tstats</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>3 rows Ã 5 columns</p><tr><th>1</th><td>gamma0</td><td>4.97442</td><td>0.0969711</td><td>0.0980515</td><td>-0.263789</td></tr><tr><th>2</th><td>gamma1</td><td>0.997909</td><td>0.0236398</td><td>0.0238491</td><td>-0.0884718</td></tr><tr><th>3</th><td>gamma2</td><td>0.161035</td><td>0.021211</td><td>0.0211003</td><td>-1.83701</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& Coef & Est & se\\_h & se\\_vs & tstats\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & gamma0 & 4.97442 & 0.0969711 & 0.0980515 & -0.263789 \\\\\n",
       "\t2 & gamma1 & 0.997909 & 0.0236398 & 0.0238491 & -0.0884718 \\\\\n",
       "\t3 & gamma2 & 0.161035 & 0.021211 & 0.0211003 & -1.83701 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "3Ã5 DataFrame\n",
       "â Row â Coef   â Est      â se_h      â se_vs     â tstats     â\n",
       "â     â \u001b[90mString\u001b[39m â \u001b[90mFloat64\u001b[39m  â \u001b[90mFloat64\u001b[39m   â \u001b[90mFloat64\u001b[39m   â \u001b[90mFloat64\u001b[39m    â\n",
       "âââââââ¼âââââââââ¼âââââââââââ¼ââââââââââââ¼ââââââââââââ¼âââââââââââââ¤\n",
       "â 1   â gamma0 â 4.97442  â 0.0969711 â 0.0980515 â -0.263789  â\n",
       "â 2   â gamma1 â 0.997909 â 0.0236398 â 0.0238491 â -0.0884718 â\n",
       "â 3   â gamma2 â 0.161035 â 0.021211  â 0.0211003 â -1.83701   â"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coef + SE table\n",
    "coef_names = [\"gamma0\"; \"gamma1\"; \"gamma2\"]\n",
    "se_h = sqrt.(diag(VCVh))\n",
    "se_vs = sqrt.(diag(VCVvs))\n",
    "tstats = (gammahat - gamma0) ./ se_h\n",
    "results = DataFrame(:Coef => coef_names, :Est => gammahat, :se_h => se_h, :se_vs => se_vs, :tstats => tstats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-val of H0: gamma = gamma0: 0.682760017633957"
     ]
    }
   ],
   "source": [
    "# Do we reject true gamma? \n",
    "ll0 = sample_log_likelihood(gamma0, data)\n",
    "LR = 2*(llhat - ll0)\n",
    "pval = cdf(Chisq(3), LR)\n",
    "print(\"P-val of H0: gamma = gamma0: $(pval)\")\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
